---
title: "Part03. 데이터 분석_1"
author: "choi"
date: '2020 11 21 '
tags:
  - "ADsP"
  - "ADSP자격증"
  - "데이터분석준전문가"
categories:
  - 
output:
  html_document:
   toc: true
   toc_float:
     collapsed: false
     smooth_scroll: true
   theme: united
   highlight: textmate 
---

- 해당 자료는 [ADsP 데이터분석 준전문가 2020 완전 개정판](http://www.dataedu.kr/product/adsp-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D-%EC%A4%80%EC%A0%84%EB%AC%B8%EA%B0%80-2020-%EC%99%84%EC%A0%84%EA%B0%9C%EC%A0%95%ED%8C%90/) 요약본으로 저작권은 DATA EDU에 있습니다.

# 1장. 데이터 분석 개요
## 1절. 데이터 분석 기법의 이해

### 1. 데이터 처리
- 데이터 분석
  - 통계 기반이나, 통계지식과 복잡한 가정이 상대적으로 적은 실용적인 분야  

- 활용
  - 대기업은 데이터 웨어하우스(DW)와 데이터마트(DM)를 통해 분석 데이터를 가져와 사용
  - 신규 시스템이나 DW에 포함되지 못한 자료는, 기존 운영 시스템(Legacy)나 스테이징 영역(staging area)과 ODS(Operational Data Store)에서 데이터를 가져와 DW에서 가져온 내용과 결합하여 활용 가능
  - 단, 운영시스템에 직접 접근해 데이터를 활용하는 것은 매우 위험 → 스테이징 영역 데이터는 운영시스템에서 임시로 저장된 데이터기에 가급적 클린징 영역인 ODS에서 데이터 전처리를 하여 DW나 DM과 결합해 사용  
  
- 최종 데이터 구조로 가공
  - **데이터 마이닝 분류**
    - 분류값과 입력변수를 연관시켜 인구통계, 요약변수, 파생벽수 등 산출  
  
  - **정형화된 패턴 처리**
    - 비정형 데이터나 소셜 데이터는 정형화된 패턴으로 처리해야 함
    
    1. **비정형 데이터**
       - DMBS에 저장됐다가 텍스트 마이닝을 거쳐 데이터 마트와 통합
       
    2. **관계형 데이터**
       - DBMS에 저장되어 사회 신경망 분석을 거쳐 분석 결과 통계값이 마트와 통합되어 활용  
       
### 2. 시각화(시각화 그래프)
- 가장 낮은 수준의 분석이지만, 제대로 사용하면 복잡한 분석보다도 효율적
- 대용량 데이터를 다루는 빅데이터 분석에서 시각화는 필수
- 탐색적 분석을 할 때, 시각화는 필수
- SNA 분석(사회연결망 분석)을 할 때, 자주 활용

### 3. 공간분석(GIS)
- **공간분석(Spatial Analysis)**: 공간적 차원과 관련된 속성을 시각화하는 분석
- 지도 위에 관련 속성을 생성하고 크기, 모양, 선, 굵기 등으로 구분하여 인사이트를 얻음

### 4. 탐색적 자료 분석(EDA)
- 탐색적 분석
  - 다양한 차원과 값을 조합하며 특이점이나 의미 있는 사실을 도출하여 분석의 최종 목적을 달성하는 과정
  - 데이터의 특징과 내재하는 구조적 관계를 알아내기 위한 기법의 통칭

- **EDA의 4가지 주제**
  - **저항성의 강조, 잔차 계산, 자료변수의 재표현, 그래프를 통한 현시성**

- 탐색적 분석 효율 예시
  -데이터 이해 단계, 변수 생성 단계, 변수 선택 단계에서 활용

### 5. 통계분석
- **통계**
  - 어떤 현상을 종합적으로 알아보기 쉽게 일정한 체계에 따라 숫자, 표, 그림 형태로 나타낸 것

- **기술통계(descriptive statistics)**
  - 모집단으로부터 표본을 추출하고 표본이 가진 정보를 쉽게 파악하도록 데이터를 정리하거나 요약하기 위해 하나의 숫자 또는 그래프 형태로 표현하는 절차

- **추측(추론)통계(inferential statistics)**
  - 모집단으로부터 추출된 표본의 표본통계량으로부터 모집단 특성인 모수에 관해 통계적으로 추론하는 절차
  
- 활용 분야
  - 정부 경제정책 수립 / 평가 근거자료(통계청 실업률, 고용률, 물가지수)
  - 농업(가뭄, 수해, 병충해에 강한 품종 개발 및 개량)
  - 의학(치료 방법의 효과나 신약 개발을 위한 임상실험 결과 분석)
  - 경영(제품 개발, 품질관리, 시장조사, 영업관리)
  - 스포츠(선수 체질향상, 경기 분석, 전략 분석, 선수 평가, 기용)

### 6. 데이터 마이닝
- 데이터 마이닝
  - 대표적인 고급 데이터 분석법
  - 대용량 자료를 요약하고 미래 예측을 목표로 자료의 관계, 패턴, 규칙을 탐색하고 모형화
  - 이전에 알려지지 않은 유용한 지식을 추출하는 분석

- 방법론
  - **데이터베이스**에서의 지식 탐색
    - 데이터 웨어하우스에서 데이터 마트를 생성하면서 각 데이터 속성을 사전분석하여 지식을 얻는 방법  
    
  - **기계학습**(machine learning)
    - 인공지능의 한 분야
    - 컴퓨터가 학습할 수 있도록 알고리즘과 기술을 개발하는 분야
    - 인공신경망, 의사결정나무, 클러스터링, 베이지안 분류, SVM 등
    
  - **패턴인식**(pattern recognition)
    - 원자료를 이용하여 사전지식, 패턴에서 추출된 통계 정보를 기반으로 자료 또는 패턴을 분류
    - 장바구니 분석, 연관 규칙
    
- 활용 분야
  - 데이터베이스 마케팅(고객 행동정보를 활용한 목표 마케팅, 고객 세분화, 장바구니 분석, 추천 시스템)
  - 신용평가 및 조기경보시스템(금융기관에서 신용카드 발급, 보험, 대출 발생 시)
  - 생물정보학(세포 유전자 분석으로 질병 진단과 치료법, 신약 개발)
  - 텍스트마이닝(전자우편, SNS 등 디지털 텍스트 정보로 고객성향, 감정, 사회관계망 분석)  


# 2장. R 프로그래밍 기초
## 1절. R 소개

### 1. 데이터 분석 도구의 현황
- R의 탄생
  - 오픈소스 프로그램으로 통계, 데이터 마이닝과 그래프를 위한 언어
  - 최신 통계 분석과 마이닝 기능 제공
  - 세계적인 사용자와 다양한 예제 공유 가능
  - 패키지가 수시로 업데이트 됨

- 분석 도구 비교

|  | SAS | SPSS | 오픈소스 R |
|---|---|---|---|
| 프로그램 비용 | 유료, 고가 | 유료, 고가 | 오픈소스 |
| 설치 용량 | 대용량 | 대용량 | 모듈화로 간단함 |
| 다양한 모듈 지원 및 비용 | 별도 구매 | 별도 구매 | 오픈 소스 |
| 최근 알고리즘 및 기술 반양 | 느림 | 다소 느림 | 매우 빠름 |
| 학습자료 입수 편의성 | 유료 도서 위주 | 유료 도서 위주 | 공개 논문 및 자료 많음 |
| 질의용 공개 커뮤니티 | NA | NA | 매우 활발 |

- R의 특징
  1. 오픈소스 프로그램
    - 사용자 커뮤니티에 도움 요청이 많음
    - 많은 패키지가 수시 업데이트
  
  2. 그래픽 및 성능
    - 프로그래밍, 그래픽 측명 등 사용 프로그램과 대등하거나 월등함
  
  3. 시스템 데이터 저장 방식
    - 각 세션 사이마다 시스템에 데이터셋을 저장 → 매번 데이터 로딩 필요가 없음
    - 명령어 스토리 저장 가능
    
  4. 모든 운영체제
    - 윈도우, 맥, 리눅스 운영체제에서 사용 가능
    
  5. 표준 플랫폼
    - S 통계 언어 기반으로 구현
    - R/S 플랫폼은 통계전문가의 사실상 표준 플랫폼
  
  6. 객체지향 언어이며 함수형 언어
    - 통계 기능뿐 아니라 일반 프로그래밍 언어처럼 자동화하거나 새로운 함수 생성 가능

    - **객체지향 언어의 특징**
      - SAS, SPSS 회귀 분석 시, 화면에 결과가 나와 추가 작업이 필요
      - R은 추정계수, 표준오차, 잔차 등 결괏값을 객체에 저장할 수 있어서 활용이 쉬움

    - **함수형 언어의 특징**
      - 깔끔하고 단축된 코드
      - 코드 실행이 빠름
      - 단순한 코드로 디버깅 노력 감소
      - 병렬 프로그래밍으로의 전환 용이
  
- R 스튜디오
  - 오픈소스이며 다양한 운영체계 지원
  - 메모리에 변수가 어떻게 되어 있는지, 타입이 무엇인지를 볼 수 있음
  - 스크립트 관리와 도큐먼테이션이 편리
  - 코딩은 스크립트용 프로그래밍으로 어렵지 않게 자동화 가능
  - 래틀(Rattle)은 GUI가 패키지와 긴밀하게 결합외어 있어 정해진 기능만 사용 가능 → 업그레이드가 제대로 되지 않으면 통합성에 문제 발생  

- R 기반 작업 환경
  - R 메모리: 64bit 유닉스- 무제한, x86 64bit- 128TB, 64bit 윈도우- 8TB
  
## 2절. R 기초
- 교재 참고

## 3절. 입력과 출력
### 1. 데이터 분석 과정
- 분석자가 분석 목적에 맞는 방법론을 선택하여 얻은 결과를 해석하는 과정
- INPUT → ANALYSIS → OUTPUT

### 2. R에서의 데이터 입력과 출력
- R에서 다룰 수 있는 파일 타입
  - Tab-delimited text, Comma-separated text, Excel file, JSON file, HTML/XML file, Database, (other) Statistical SW's file

## 4절. 데이터 구조와 데이터 프레임 1
### 1. 백터(Vector)
- 백터들은 동질적
 - 한 백터의 모든 원소는 같은 자료형 또는 같은 모드(mode)를 가짐

- 백터는 위치로 인덱스 됨
  - V[2]는 V 백터의 2번째 원소

- 백터는 인덱스를 통해 여러 개 원소로 구성된 하위 백터를 반환할 수 있음
  - V[c(2,3)]은 V 백터의 2번째, 3번째 원소로 구성된 하위 백터

- 백터 원소들은 이름을 가질 수 있음
  ```
  V <- c(10,20,30); names(v) <- c("Moe", "Larry", "Curly")
  v["Larry"]  
  Larry  
  20
  ```

### 2. 리스트(Lists)
- 리스트는 이질적
  - 여러 자료형 원소가 포함될 수 있음

- 리스트는 위치로 인덱스 됨
  - L[[2]]는 L 리스트의 2번째 원소

- 리스트에서 하위 리스트 추출 가능
  - L[c(2,3)]은 L 리스트의 2번째, 3번째 원소로 이루어진 하위 리스트

- 리스트의 원소들은 이름을 가질 수 있음
  - L[["Moe"]]와 L$Moe는 둘 다 "Moe"라는 이름의 원소를 지칭

### 3. R에서의 자료 형태(mode)

| 객체 | 예시 | 모드 |
|---|---|---|
| **숫자** | 3.1415 | 수치형(numeric) |
| **숫자 백터** | c(2,3,4,5,5) | 수치형(numeric) |
| **문자열** | "Tom" | 문자형(character) |
| **문자열 백터** | c("Tom","Yoon","Kim") | 문자형(character) |
| **요인** | factor(c("A","B","C")) | **수치형(numeric)** |
| **리스트** | list("Tom","Yoon","Kim") | 리스트(list) |
| **데이터 프레임** | data.frame(x=1:3, y=c("Tom","Yoon","Kim")) | 리스트(list) |
| **함수** | print | 함수(function) |

### 4. 데이터 프레임(data frames)
- 강력하고 유연한 구조, SAS 데이터셋을 모방해서 만들어짐
- 데이터 프레임의 리스트 원소는 백터 또는 요인
- 백터와 요인은 데이터 프레임의 열
- 백터와 요인은 동일한 길이
- 데이터 프레임은 표 형태의 데이터 구조, 열별로 다른 데이터 형식을 가질 수 있음
- 열에는 이름이 있어야 함

- 데이터 프레임 원소 접근 방법
  ```
  b[1]; b["empno"]
    b[[i]]; b[["empno"]]
    b$empno
  ```

### 5. 그밖의 데이터 구조
- 단일값(Scalars)
  - R에서는 원소가 하나인 백터로 인식/처리
```{r}
pi
length(pi)
```

- 행렬(Matrix)
  - R에서는 차원을 가진 백터로 인식
```{r}
a <- 1:9
dim(a) <- c(3,3)
a
```

- 배열(Arrays)
  - 행렬에 3차원 또는 n차원까지 확장된 형태
  - 주어진 백터에 더 많은 차원을 부여해 배열 생성
```{r}
b <- 1:12
dim(b) <- c(2,3,2)
```

- 요인(Factors)
  - 백터처럼 생겼지만, R에서는 백터에 있는 고유값(unique key) 정보를 얻는데, 고유값들을 요인의 수준(level)이라고 함
  - 요인의 주된 2가지 사용처: 범주형 변수, 집단 분류

### 6. 백터, 리스트, 행렬 다루기
- 행렬(Matrix)은 R에서 차원을 가진 백터이며, 텍스트마이닝과 소셜 네트워크 분석 등에 활용
- 재활용 규칙(Recycling Rule)
  - 길이가 다른 두 백터 연산을 할 때, R은 짧은 백터의 처음으로 돌아가 연산이 끝날 때까지 원소를 재활용

# 3장. 데이터 마트
## 1절. 데이터 변경 및 요약
### 1. R reshape를 이용한 데이터 마트 개발
- **데이터 마트**
  - **데이터 웨어하우스와 사용자 사이의 중간층**에 위치한 것
  - 하나의 주제 또는 하나의 부서 중심의 데이터 웨어하우스
  - 데이터 마트 내 대부분의 데이터는 데이터 웨어하우스로부터 복제
  - 또는 자체적으로 수집되거나 관계형/다차원 데이터 베이스를 이용해 구축
  - CRM 관련 업무 중 핵심: 고객 데이터 마트 구축
  - 동일한 데이터셋 활용 시, 데이터 마트를 어떻게 구축하느냐에 분석 효과 차이를 만듦

- **요약변수**
  - 수집된 정보를 분석에 맞게 종합한 변수
  - 가장 기본적인 변수로 총 구매 금액, 금액, 횟수, 구매여부 등 데이터 분석을 위해 만들어지는 변수
  - 많은 모델을 공통으로 사용할 수 있어 재활용성이 높음
  - 합계, 횟수와 같이 간단한 구조이므로 자동화하여 구측 가능
  - **단점**: 얼마 이상이면 구매하더라도 기준값 의미 해석이 애매할 수 있음 → 연속형 변수를 그룹핑해 사용하는 것이 좋음

- **파생변수**
  - 사용자(분석자)가 특정 조건을 만족하거나 특정 함수에 의해 값을 만들어 의미를 부여한 변수
  - 매우 주관적일 수 있으므로 논리적 타당성을 갖추어 개발해야 함
  - 세분화, 고객행동 예측, 캠페인 반응 예측에 활용
  - 파생변수는 상황에 따라 특정 상황에만 유의미하지 않고 대표성을 띄게 해야 함

- **reshape의 활용**
  - reshape 패키지에는 melt()와 cast()라는 2개 핵심 함수가 있음
    - **melt(): 쉬운 casting을 위해 적당한 형태로 만들어주는 함수**
    - melt(data, id=...)
    - **cast(): 데이터를 원하는 형태로 계산, 변형하는 함수**
    - cast(data, formula=... ~ variable, fun)
  - 변수를 조합해 변수명을 만들고 변수를 시간, 상품 등 차원과 결합해 다양한 요약변수와 파생변수를 쉽게 생성하여 데이터 마트를 구성할 수 있게 함

### 2. sqldf를 이용한 데이터 분석
- sqldf는 R에서 sql 명령어를 사용 가능하게 하는 패키지
- SAS에서의 proc sql과 같은 역할을 하는 패키지
- 명령어 차이(sql, R)
  - sql: select * from [data frame], R: sqldf("select * from [data frame]")
  - sql: select * from [data frame] numrows 10, R: sqldf("select * from [data frame] limit 10")
  - sql: select * from [data frame] where [col] = 'char%', R: sqldf("select * from [data frame] where [col] like 'char%' ")

### 3. plyr을 이용한 데이터 분석
- apply 함수에 기반해 데이터와 출력변수를 동시에 배열로 치환하여 처리하는 패키지
- split - apply - combine: 데이터 분리, 처리, 결합 등 필수적인 처리 기능 제공

|  | array | data frame | list | nothing |
|---|---|---|---|---|
| array | aaply | adply | alply | a_ply |
| data frame | daply | ddply | dlply| d_ply |
| list | laply | ldply | llply | l_ply |
| n replicates | raply | rdply | rlply | r_ply |
| function arguments | maply | mdply | mlply | m_ply |

### 4. 데이터 테이블
- data.table 패키지는 R에서 가장 많이 사용하는 데이터 핸들링 패키지 중 하나
- data.table은 큰 데이터를 탐색, 연산, 병합하는 데 유용
- 기존 data.frame 방식보다 월등히 빠른 속도
- 특정 column을 key 값으로 색인 지정 후, 데이터 처리
- 빠른 그루핑과 ordering, 짧은 문장 지원 측면에서 데이터프레임보다 유용(속도차 큼)

## 2절. 데이터 가공
### 1. Data Exploration
- Data Exploration
  - 데이터 분석을 위해 구성된 데이터 변수들의 상태를 파악

- 종류
  1. head(데이터셋), tail(데이터셋)
  2. summary(데이터셋)
     - 수치형변수: 최대값, 최소값, 평균, 1사분위수, 2사분위수(중앙값), 3사분위수
     - 명목형변수: 명목값, 데이터 개수
    
### 2. 변수 중요도
- 변수 중요도
  - 변수 선택법과 유사한 개념으로 모형을 생성하여 사용된 변수의 중요도를 살피는 과정

- 종류
  - klaR 패키지
    - 특정 변수가 주어졌을 때, 클래스가 어떻게 분류되는지에 관한 에러율을 계산하고 그래픽으로 결과를 보여주는 기능
    - greedy.wilks(): 세분화를 위한 stepwise forward 변수 선택을 위한 패키지, 종속변수에 가장 영향력을 미치는 변수를 wilks lambda를 활용하여 변수 중요도 정리 **(Wilk's Lambda = 집단내분산/총분산)**

### 3. 변수의 구간화
- 변수의 구간화
  - **연속형 변수**를 분석 목적에 맞게 활용하기 위해 구간화하여 모델링에 적용
  - 일반적으로 10진수 단위로 구간화, 구간을 5개로 나누는 것이 보통이며 7개 이상의 구간을 만들지 않음
  - 신용 평가 모형, 고객 세분화 같은 시스템에서 모형에 활용하는 각 변수를 구간화해서 구간별로 점수를 적용하는 스코어링 방식으로 활용

- 구간화 방법
  1. binning
     - 신용평가모형 개발에서 연속형 변수(부채비율 등)를 범주형 변수로 구간화하는데 자주 활용

  2. 의사결정나무
     - 세분화 또는 예측에 활용되는 의사결정나무 모형을 사용해 입력변수 구간화 가능
     - 동일한 변수를 여러 번의 분리 기준으로 사용 가능하기 때문에, 연속변수가 반복적으로 선택될 경우 각각 분리 기준값으로 연속형 변수를 구간화할 수 있음

## 3절. 기초 분석 및 데이터 관리
### 1. 데이터 EDA(탐색적 자료 분석)
- 데이터 분석에 앞서 전체적으로 데이터 특징을 파악하고 다양한 각도로 데이터에 접근
- summary()를 이용해 데이터의 기초통계량 확인

### 2. 결측값 인식
- 결측값은 NA, 99999999, ' '(공백), Not Answer 등으로 표현
- 결측값 자체에 의미가 있는 경우도 있음: 쇼핑몰 중 특정 거래 자체가 존재하지 않는 경우, 아주 부자이거나 아주 가난한 경우 정보를 잘 채우지 않음
- 결측값 처리는 전체 작업속도에 많은 영향을 줌

### 3. 결측값 처리 방법
- **단순 대치법(Single Imputation)**
  1. completes analysis
     - 결측값이 존재하는 레코드 삭제
  
  2. 평균 대치법(Mean Imputation)
     - 관측 또는 실험을 통해 얻어진 데이터의 평균으로 대치
     - 비조건부 평균 대치법: 관측 데이터 평균으로 대치
     - 조건부 평균 대치법(regression imputation): 회귀분석을 활용한 대치법
  
  3. 단순확률 대치법(Single Stochastic Imputation)
     - 평균 대치법에서 추정량 표준 오차의 과소 추정문제를 보완하고자 고안된 방법
     - Hot-deck 방법, nearest neighbor 방법 등

- **다중 대치법(Multiple Imputation)**
  - m번의 대치를 통해 m개의 가상적 완전 자료를 만드는 방법
  - 1단계: 대치(imputation step), 2단계: 분석(Analysis step), 3단계: 결합(combination step)
  - Amelia-time series cross sectional data set(여러 국가에서 매년 측정된 자료)에서 boostrapping based algorithm을 활용한 다중 대치법

### 4. R에서 결측값 처리
- 관련 함수

| 함수 | 내용 | 
|---|---|
| complete.cases() | 데이터 내 레코드에 결측값 있으면 FALSE, 있으면 TRUE로 반환 |
| is.na() | 결측값을 NA로 인식하여 결측값 있으면 TRUE, 없으면 FALSE로 반환 |
| DMwR 패키지의 centrallmputation() | NA 값에 가운데 값(central value)으로 대치, 숫자는 중위수, 요인(factor)은 최빈값으로 대치 |
| DMwR 패키지의 knnlmputation | NA 값을 k 최근 이웃 분류 알고리즘을 사용하여 대치, k개 주변 이웃까지의 거리를 고려하여 가중 평균한 값 사용 |
| Amelia 패키지의 amelia() | time-series-cross-sectional data set에서 활용(랜덤포레스트 모델은 결측값 돈재할 경우 바로 에러 발생), ramdomForest 패키지의 rflmpute() 함수를 활용해 NA 결측값을 대치한 후 알고리즘에 적용 |

### 5. 이상값(Outlier) 인식과 처리
- 이상값이란?
  - 의도하지 않게 잘못 입력한 경우 (Bad data)
  - 의도하지 않게 입력되었으나 분석 목접에 부합되지 않아 제거해야 하는 경우 (Bad data)
  - 의도하지 않은 현상이지만 분석에 포함해야 하는 경우
  - 의도된 이상값(fraud, 불량)인 경우
  - 이상값을 꼭 제거해야 하는 것은 아님

- 이상값 인식 방법
  1. **ESD(Extreme Studentized Deviation)**
     - 평균으로부터 3 표준편차 떨어진 값(각 0.15%)
  
  2. 기하평균 -2.5 x 표준편차 < data < 기하평균 +2.5 x 표준편차
  
  3. 사분위수 이용하여 제거하기(상자 그림의 outer fence 밖에 있는 값 제거)
     - 이상값 정의: Q1 - 1.5(Q3 - Q1) < data < Q3 + 1.5(Q3 - Q1)을 벗어나는 데이터

- **극단값 절단(trimming) 방법**
  1. 기하평균을 이용한 제거
     - geo_mean
  
  2. 하단, 상단 % 이용한 제거
     - 10% 절단(상하위 5%에 해당되는 데이터 제거)

- **극단값 조정(winsorizing) 방법**
  - 상한값과 하한값을 벗어나는 값들을 상한, 하한값으로 바꾸어 활용
  