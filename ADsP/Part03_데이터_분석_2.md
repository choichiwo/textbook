---
title: "Part03. 데이터 분석_2"
author: "choi"
date: '2020 11 22 '
tags:
  - "ADsP"
  - "ADSP자격증"
  - "데이터분석준전문가"
categories:
  - 
output:
  html_document:
   toc: true
   toc_float:
     collapsed: false
     smooth_scroll: true
   theme: united
   highlight: textmate 
---

- 해당 자료는 [ADsP 데이터분석 준전문가 2020 완전 개정판](http://www.dataedu.kr/product/adsp-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D-%EC%A4%80%EC%A0%84%EB%AC%B8%EA%B0%80-2020-%EC%99%84%EC%A0%84%EA%B0%9C%EC%A0%95%ED%8C%90/) 요약본으로 저작권은 DATA EDU에 있습니다.

# 4장. 통계 분석
## 1절. 통계분석의 이해

### 1. 통계
- 특정집단을 대상으로 수행한 조사 / 실험 결과의 요약된 형태
- 조사 대상에 따라 총조사(census)와 표본조사로 구분

### 2. 통계자료의 획득 방법
- **총 조사(전수 조사, census)**
  - 대상 집단 모두를 조사하면 많은 비용과 시간이 소요되므로 특별한 경우를 제외하고는 사용되지 않음

- **표본조사**
  - 모집단에서 샘플을 추출하여 진행하는 조사
  - **모집단(population)**: 조사하고자 하는 대상 집단 전체
  - **원소(element)**: 모집단을 구성하는 개체
  - **표본(sample)**: 조사하기 위해 추출한 모집단의 일부 원소
  - **모수(parameter)**: 표본 관측에 의해 구하고자 하는 모집단에 대한 정보
  - 모집단의 정의, 표본 크기, 조사 방법, 조사 기간, 표본 추출 방법을 정확히 명시해야 함

- 표본 추출 방법
  1. **단순 랜덤 추출법(simple random sampling)**
     - 각 샘플에 번호를 부여해 임의의 n개를 추출하는 방법
     - 각 샘플이 선택될 확률은 동일(비복원, 복원(추출 element를 다시 집어넣음) 추출)
  
  2. **계통추출법(systematic sampling)**
     - 단순랜덤추출법의 변형된 방식
     - 임의 위치에서 매 k번째 행목을 추출하는 방법
     - 번호를 부여한 샘플을 나열하여 K개씩 (K = N/n) n개의 구간으로 나누고 첫 구간(1, 2, ..., K)에서 하나를 임의로 선택한 후 K개씩 띄어서 n개의 표본을 선택
     
  3. **집락추출법(cluster random sampling)**
     - 군집을 구분하고 군집별로 단순랜덤 추출법을 수행한 후, 모든 자료를 활용하거나 샘플링 하는 방법
     - 지역 표본 추출, 다단계 표본 추출
     
  4. **층화추출법(stratified random sampling)**
     - 이질적인 원소들로 구성된 모집단에서 각 계층을 대표할 수 있도록 표본을 추출하는 방법
     - 유사한 원소끼리 몇 개의 층(stratum)으로 나누어 각 층에서 랜덤 추출하는 방법
     - 비례층화추출법, 불비례층화추출법

- **측정(measurement)**
  - 측정
    - 표본조사나 실험 과정에서 추출된 원소들이나 실험 단위로부터 주어진 목적에 적합하도록 관측하여 자료를 얻는 것
  
  - 측정 방법
  
  |  | 내용 |
  |---|---|
  | 명목척도 | 측정 대상이 어느 집단에 속하는지 분류할 때 사용 (성별, 출생지 구분) |
  | 순서척도 | 측정 대상의 서열관계를 관측하는 척도 (만족도, 선호도, 학년, 신용등급) |
  | 구간척도(등간척도) | 측정 대상이 갖는 속성의 양을 측정, 구간이나 구간 사이 간격에 의미 있는 자료 (온도, 지수) +,- 가능 *,/ 불가능 |
  | 비율척도 | 간격(차이) 비율이 의미를 가지는 자료, 절대적 기준인 0이 존재하고 사칙연산 가능, 제일 많은 정보를 가지는 척도 (무게, 나이, 시간, 거리) |
  
  - 질적 척도: 명목척도, 순서척도 → 범주형 자료, 숫자 크기 차이가 계산되지 않는 척도
  - 양적 척도: 구간척도, 비율척도 → 수치형 자료, 숫자 크기 차이를 계산할 수 있는 척도

### 3. 통계분석
- 통계분석
  - 특정한 집단이나 불확실한 현상을 대상으로, 자료를 수집해 대상 집단 정보를 구하고 통계분석 방법을 이용하여 의사결정 하는 과정
  
- **기술통계(descriptive statistic)**
  - 주어진 자료로부터 어떠한 판단이나 예측과 같은 주관이 섞일 수 있는 과정을 배제하여 통계집단의 특성을 수량화하여 객관적인 데이터로 나타내는 통계분석 방법론
  - sample에 대한 특성인 평균, 표준편차, 중위수, 최빈값, 그래프, 왜도, 첨도 등을 구하는 것

- **통계적 추론(추측통계, inference statistics)**
  - 수집된 자료를 이용해 대상 집단(모집단)에 관한 의사결정을 하는 것
  - sample을 통해 모집단을 추정
  
  1. **모수추정**
     - 표본집단으로부터 모집단의 특성인 모수(평균, 분산 등)를 분석하여 모집단 추론
  
  2. **가설검정**
     - 대상집단에 관해 특정한 가설을 설정한 후에 가설 채택 여부를 결정하는 방법론
     
  3. **예측**
     - 미래의 불확실성을 해결해 효율적인 의사결정을 하기 위해 활용
     - 예. 회귀분석, 시계열분석 등

### 4. 확률 및 확률분포
- 확률
  - 표본공간 S에 부분집합인 각 사상에 대해 실수값을 가지는 함수의 확률값이 0과 1 사이에 있고 전체 확률의 합이 1인 것을 의미
  - 표본공간 Ω의 부분집합인 사건 E의 확률은 표본공간의 원소 개수에 대한 사건 E 개수의 비율로 확률을 P(E)라고 할 때, 다음과 같의 정의
  - P(E) = $\frac[n(E)][N(Ω)]$
  
  1. **표본공간(sample space, Ω)**
     - 어떤 실험을 실시할 때 나타날 수 있는 모든 결과 집합
     
  2. **사건(event)**
     - 관찰자가 관심 있는 사건, 표본공간의 부분집합
     
  3. **원소(element)**
     - 나타날 수 있는 개별 결과
  
  4. **확률변수(random variable)**
     - 특정값이 나타날 가능성이 확률적으로 주어지는 변수
     - 정의역(domain)이 표본공간, 치역(range)이 실수값 (0 < y < 1)인 함수
     - 0이 아닌 확률을 갖는 실수값이 형태에 따라, 이산형 확률변수(discrete random variable)와 연속형 확률변수(continuous random variable)로 구분

     - **덧셈정리(배반 X)**
       - 사건 A와 사건 B가 동시에 일어날 수 있을 때(교집합 성립)
       - 일어날 확률 P(A 또는 B): P(A∪B) = P(A) + P(B) - P(A∩B)
       - 사건 B가 주어졌을 때, 사건 A의 조건부확률: **P(A|B) = P(A∩B)/P(B)**
      
     - **덧셈정리(배반 O)**
       - 사건 A와 사건 B가 동시에 일어나지 않을 때
       - 사건 A or 사건 B 중, 한 쪽만 일어날 확률: **P(A∪B) = P(A) + P(B)**
    
     - **곱셈정리**
       - 사건 A와 B가 서로 무관계하게 나타날 때(독립사건)
       - 사건 B가 주어졌을 때, 사건 A의 조건부확률: **P(A|B) = P(A)**
       
- 확률분포
  - **이산형 확률 변수**
    1. **베르누이 확률분포**(Bernoulli distribution)
       - 결과가 2개만 나오는 경우
       - 동전 던지기, 시험의 합격/불합격, 안타를 칠 확률
    
    2. **이항분포**(Binomial distribution)
       - 베르누이 시행을 n번 반복했을 때, k번 성공할 확률
       - 5번 타석에 들어와서 3번 안타를 칠 확률 → n=5, k=3, 안타를 칠 확률 P(x)=타율
       - 성공할 확률 P가 0이나 1에 가깝지 않고 n이 충분히 크면 정규분포에 가까워짐, 1/2에 가까우면 종 모양
    
    3. **기하분포**(Geometric distribution)
       - 성공확률이 p인 베르누이 시행에서 첫 번째 성공이 있기까지 x번 실패할 확률
       - 5번 타석에 들어와서 3번째 타석에서 안타를 칠 확률
      
    4. **다항분포**(Multinomial distribution)
       - 세 가지 이상의 결과를 가지는 반복 시행에서 발생하는 확률 분포 (이항분포 확장한 것)
       
    5. **포아송분포**(Poisson distribution)
       - 시간과 공간 내에서 발생하는 사건의 발생횟수에 대한 확률분포
       - 책에 오타가 5p당 10개 나온다고 할 때, 한 페이지에 오타가 3개 나올 확률
       - 최근 5경기에서 10개의 홈런을 쳤다고 할 때, 오늘 경기에서 홈런을 치지 못할 확률
    
  - **연속형 확률 변수**
    1. **균일분포**(일양분포, Uniform distribution)
       - 모든 확률변수 X가 균일한 확률을 가지는 확률분포 (다트의 확률분포)
    
    2. **정규분포**(Normal distribution)
       - 평균이 μ이고 표준편차가 σ인 X의 확률밀도함수
       - 표준편차가 클 경우 그래프가 퍼져보임
       - 표준정규분포: 평균 0, 표준편차 1 → 정규분포를 표준정규분포로 만드는 공식: z = $\frac{X-μ}{σ}$
    
    3. **지수분포**(Exponential distribution)
       - 어떤 사건이 발생할 때까지 경과 시간에 대한 연속확률분포
       - 전자레인지 수명 시간, 콜센터에 전화가 걸려올 때까지의 시간, 은행 고객 내방에 걸리는 시간, 버스가 올 때까지 시간
    
    4. **t-분포**(t-distribution)
       - 데이터가 연속형일 때, 두 집단 평균이 동일한지 알고 싶을 때 사용
       - 평균이 0을 중심으로 좌우가 동일한 분포
       - 정규분포보다 퍼져 있고 자유도가 커질수록 정규분포에 가까워짐
    
    5. **X^2^-분포**(chi-square distribution)
       - 두 집단 간의 동질성 검정에 활용
       - 범주형 자료에 얻어진 관측값과 기대값 차이를 보는 적합성 검정에 활용
       - 모평균과 모분산이 알려지지 않은 모집단의 모분산에 대한 가설 검정에 사용되는 분포
    
    6 **F-분포**(F-distribution)
       - 두 집단 간 분산의 동일성 검정에 사용
       - 확률변수는 항상 양의 값만 갖고 x^2^-분포와 달리 자유도를 2개 가지며 자유도가 커질수록 정규분포에 가까워짐
       
### 5. 추정과 가설 검정
- 추정의 개요
  - **확률표본**(random sample)
    - 확률분포는 분포를 결정하는 평균, 분산 등 모수(parameter)를 가지고 있음
    - 특정한 확률분포로부터 독립적으로 반복해 표본을 추출하는 것
    - 각 관찰값들은 서로 독립적이며 동일한 분포를 가짐
  
  - **추정**
    - 표본으로부터 미지의 모수를 추측하는 것
    
    1. **점추정**(point estimation)
       - **'모수가 특정한 값일 것**'이라고 추정하는 것
       - 표본의 평균, 중위수, 최빈값 등을 사용
       - 점추정량의 조건, 표본평균, 분산
         - 불편성: 가능한 표본에서 얻은 추정량의 기대값은 모집단의 모수와 편의(차이)가 없음
         - 효율성: 추정량의 분산이 작을수록 좋음
         - 일치성: 표본 크기가 아주 커지면, 추정량이 모수와 거의 같아짐
         - 충족성: 추정량은 모수에 대해 모든 정보를 제공
         - 표본평균: 모집단 평균(모평균)을 추정하기 위한 추정량, 확률표본의 평균값
         - 표본분산: 모집단의 분산(모분산)을 추정하기 위한 추정량
    
    2. **구간추정**(interval estimation)
       - 점추정의 정확성을 보완하기 위해, 확률로 표현된 믿음의 정도 하에서 **모수가 특정한 구간에 있을 것**이라고 선언하는 것
       - 항상 추정량 분포에 대한 전제와, 구해진 구간 안에 모수가 있을 가능성의 크기(신뢰수준(confidence interval))가 주어져야 함
       - 참고: 모분산을 알 때는 분자에 σ, 모를 때는 S를 넣음

- 가설검정
  - 모집단에 대한 가설을 설정하고, 표본관찰을 통해 가설의 채택여부를 결정하는 분석 방법
  - 표본 관찰 또는 실험을 통해 귀무가설과 대립가설 중 하나를 선택
  - 귀무가설이 옳다는 전제 하에 검정통계량 값을 구하고, 이 값이 나타날 가능성의 크기에 의해 귀무가설 채택 여부를 결정
  
  1. **귀무가설(null hypothesis, H~0~)**
     - '비교하는 값과 차이가 없다, 동일하다'를 기본개념으로 하는 가설
     
  2. **대립가설(alternative hypothesis, H~1~)**
     - 뚜렷한 증거가 있을 때 주장하는 가설
     
  3. **검정통계량(test statistic)**
     - 관찰된 표본으로부터 구하는 통계량, 검정 시 가설 진위를 판단하는 기준
  
  4. **유의수준(significance level, α)**
     - 귀무가설이 옳은데도 기각하는 확률 크기
     
  5. **기각역(critical regoin, C)**
     - 귀무가설이 옳다는 전제 하에서 구한 검정통계량 분포에서, 확률이 유의수준 α인 부분
     - 반대는 채택역(acceptance region)
  
  - 제1종 오류와 제2종 오류
  
  | 사실 \ 가설검정 결과 | H~0~가 사실이라고 판정 | H~0~가 사실 아니라고 판정 |
  |---|---|---|
  | H~0~가 사실 | 옳은 결정 | 제1종 오류(α) |
  | H~0~가 사실 아님 | 제2종 오류(β) | 옳은 결정 |
  
  - 두 가지 오류는 상충관계라, 가설검정에서는 제1종 오류 크기를 0.1, 0.05, 0.01 등으로 고정한 뒤, 제2종 오류가 최소가 되도록 기각역을 설정함

### 6. 비모수 검정
- **모수적 방법**
  - 검정하고자 하는 모집단의 분포에 대해 가정하고, 가정 하에서 검정통계량과 검정통계량 분포를 유도해 검정 실시
  
- **비모수적 방법**
  - 자료가 추출된 모집단의 분포에 대한 아무 제약을 가하지 않고 검정 실시
  - 관측된 자료가 특정분포를 따른다고 가정할 수 없는 경우에 이용
  - 관측된 자료 수가 많지 않거나(30개 미만) 자료가 개체 간의 서열관계를 나타내는 경우에 이용
  
- **모수적 검정 vs 비모수적 검정**
  1. 가설의 설정
     - 모수적 검정: 가정된 분포의 모수에 대해 가설 설정
     - 비모수적 검정: 가정된 분포 x → 가설은 단지 분포의 형태가 동일하다/동일하지 않다'처럼 분포 형태를 설명
  
  2. 검정 방법
     - 모수적 검정: 관측된 자료로 구한 표본평균, 표본분산 등 이용해 검정
     - 비모수적 검정: 관측값의 절대적 크기에 의존하지 않는 관측값의 순위나 두 관측값 차이의 부호 등 이용해 검정

- 비모수적 검정의 예
  - 부호 검정, 윌콕슨의 순위합검정, 윌콕슨의 부호순위합검정, 만-위트니의 U검정, 런검정, 스피어만의 순위상관계수


## 2절. 기초 통계분석
### 1. 기술통계
- **기술통계(Descriptive Statistics)**
  - 자료 특성을 그림, 통계량을 사용해 쉽게 파악할 수 있도록 정리하는 것
  - 자료를 요약하는 기초적 통계를 의미
  - 데이터 분석에 앞서 대략적 통계적 수치를 계산 → 통찰력 얻기에 유리

- 통계량에 이한 자료 정리
  1. 중심위치의 측도
  2. 산포의 측도: 분산, 표준편차, 범위, 사분위수 범위 등

- 분포 형태에 관한 측도
  - 왜도: 분포의 비대칭 정도를 나타내는 측도
    - m~3~ > 0: 오른쪽으로 긴 꼬리를 갖는 분포 (최빈값 < 중앙값 < 평균)
    - m~3~ = 0: 좌우가 대칭인 분포
    - m~3~ < 0: 왼쪽으로 긴 꼬리를 갖는 분포 (평균 < 중앙값 < 최빈값)

- 그래프를 이용한 자료 정리
  - 히스토그램
    - 표로 된 도수분포를 그림으로 나타낸 것
  
  - **막대그래프 vs 히스토그램**
    - 막대그래프
      - 범주(category)형으로 구분된 데이터를 표현 → 의도에 따라 범주의 순서를 바꿀 수 있음
      - 직업, 종교, 음식
      
    - 히스토그램
      - 연속(continuous)형으로 표시된 데이터 → 임의로 순서를 바꿀 수 없고 막대의 간격이 없음
      - 몸무게, 성적, 연봉

  - 히스토그램의 생성
    - 계급의 수는 2^k^ ≥ n을 만족하는 최소의 정수 log~2~n = k에서 최소의 정수
    - 계급 간격은 $\frac{(최대값 - 최소값)}{계급수}$로 파악 가능
    - 계급 수와 간격이 변하면 히스토그램 모양도 변함
  
  - 줄기-잎 그림(stem-and leaf plot)
  
  - 상자그림(Box plot)
    - 다섯 숫자 요약을 통해 그림으로 표현(최소값, Q1, Q2, Q3, 최대값)
    - 사분위수 범위(IQR): Q3 - Q1
    - 안울타리(inner fence): Q1 - 1.5 X IQR 또는 Q3 + 1.5 X IQR
    - 바깥울타리(outer fence): Q1 - 3 X IQR 또는 Q3 + 3 X IQR
    - 보통이상점(mild outlier): 안쪽울타리와 바깥울타리 사이 자료
    - 극단이상점(extreme outlier): 바깥울타리 밖 자료

### 2. 인과관계의 이해
- 용어
  - 종속변수(반응변수, y)
    - 다른 변수의 영향을 받는 변수

  - 독립변수(설명변수, x)
    - 영향을 주는 변수

  - 산점도(sxatter plot)
    - 좌표평면 위에 점들로 표현한 그래프

- 공분산(covariance)
  - 두 확률변수 X, Y 방향의 조합(선형성)
  - 공분산 부호로 두 변수의 방향성 확인 가능
    - **공분산 부호가 +: 두 변수는 양의 방향성, 공분산 부호가 -: 두 변수는 음의 방향성**을 가짐
    - X, Y가 서로 독립이면, Cov(X,Y) = 0

### 3. 상관분석
- 상관분석(Correlation Analysis)
  - 두 변수 간 관계의 정도를 알아보기 위한 분석 방법
  - 상관계수(Correlation coefficient)이용

- 상관관계 특성

  | 상관계수 범위 | 해석 |
  |---|---|
  | 0.7 < r ≤ 1 | 강한 양(+)의 상관이 있다 |
  | 0.3 < r ≤ 0.7 | 약한 양(+)의 상관이 있다 |
  | 0 < r ≤ 0.3 | 거의 상관이 없다 |
  | r = 0 | 상관관계(선형, 직선)가 존재하지 않는다 |
  | -0.3 ≤ r < 0 | 거의 상관이 없다 |
  | -0.7 ≤ r < -0.3 | 약한 음(-)의 상관이 있다 |
  | -1 ≤ r < -0.7 | 강한 음(-)의 상관이 있다 |

- 상관분석 유형

  | 구분 | 피어슨 | 스피어만 |
  |---|---|---|
  | 개념 | 등간척도 이상으로 측정된 두 변수의 상관관계 측정 방식 | 서열척도인 두 변수 상관관계 측정 방식 |
  | 특징 | 연속형 변수, 정규성 가정, 대부분 많이 사용 | 순서형 변수, 비모수적 방법, 순위 기준 상관관계 측정 |
  | 상관계수 | 피어슨 r(적률상관계수) | 순위상관계수(p, 로우) |

- 상관분석을 위한 R
  - 분산: var(x,y=NULL, na.rm=FALSE)
  - 공분산: cov(x,y=NULL, use="everything", method=c("pearson", "kendall", "spearman"))
  - 상관관계: cor(x,y=NULL, use="everything", method=c("pearson", "kendall", "spearman"))
  - 상관관계(Hmisc 패키지): rcorr(matrix(data명), type=c("pearson", "kendall", "spearman"))

- **상관분석의 가설 검정**
  - 상관계수 r이 0이면 입력변수 x와 출력변수 y사이에는 아무런 관계가 없음 (귀무가설: r=0, 대립가설: r≠0)
  - t 검정통계량을 통해 얻은 p-value값이 0.05 이하인 경우, 대립가설을 채택하게 되어 데이터에서 구한 상관계수를 활용할 수 있게 됨

- **상관분석 예제**
  - cov: 공분산
  - cor: 상관계수
  - p-value: 유의수준 0.05보다 작게 나타나면 상관계수가 있음


## 3절. 회귀분석
### 1. 회귀분석
- 회귀분석
  - 하나나 그 이상의 독립변수들이 종속변수에 미치는 영향을 추정할 수 있는 통계기법
  - 변수 사이의 인과관계를 밝히고 모형을 적합하여 관심 있는 변수를 예측하거나 추론하기 위한 분석 방법
  - 독립변수의 개수가 하나면 단순선형회귀분석, 독립변수 개수가 두 개 이상이면 다중선형회귀분석

- 회귀분석의 변수
  - 영향 받는 변수(y): 반응변수(response variable), 종속변수(dependent variable), 결과변수(outcome variable)
  - 영향 주는 변수(x): 설명변수(explanatory variable), 독립변수(independent variable), 예측변수(predictor variable)

- 선형회귀분석의 가정
  1. **선형성**
     - 입력변수와 출력변수의 관계가 선형 (가장 중요한 가정)
  
  2. **등분산성**
     - 오차 분산이 입력변수와 무관하게 일정
     - 잔차플롯(산점도)를 활용해 잔차와 입력변수 간 아무런 관련성이 없게 무작위적으로 고루 분포돼야 등분산성 가정 만족
  
  3. **독립성**
     - 입력변수와 오차는 관련 없음
     - 자기상관(독립성)을 알아보기 위해 Durbin-Waston 통계량 사용
     - 시계열 데이터에서 많이 활용
  
  4. **비상관성**
     - 오차들끼리 상관이 없음
  
  5. **정상성(정규성)**
     - 오차 분포가 정규분포를 따름
     - Q-Q plot, Kolmogolov-Sirnov 검정, Shaprio-Wilk 검정 등 활용

- 가정에 대한 검증
  - 단순선형회귀분석
    - 입력변수와 출력변수 간 선형성을 점검하기 위해 산점도 확인
  
  - 다중선형회귀분석
    - 선형회귀분석 가정인 선형성, 등분산성, 독립성, 정상성이 모두 만족하는지 확인

### 2. 단순선형회귀분석
- 하나의 독립변수가 종속변수에 미치는 영향을 추정할 수 있는 통계기법

- 회귀분석 검토사항
  1. 회귀계수가 유의미한가?
     - 해당 계수의 t 통계량의 p-값이 0.05보다 작으면 해당 회귀계수가 통계적으로 유의하다고 볼 수 있음
  
  2. 모형이 설명력을 갖는가?
     - 결정계수(R^2^)를 확인
     - 결정계수는 0~1값을 가지며, 높을수록 추정된 회귀식의 설명력이 높아짐
  
  3. 모형이 데이터를 잘 적합하는가?
     - 잔차를 그래프로 그리고 회귀진단

- 회귀계수의 추정
  - 최소제곱법, 최소자승법
  - 측정값을 기초로 적당한 제곱합을 만들고 이를 최소로 하는 값을 구해 측정결과를 처리
  - 잔차제곱이 가장 작은 선을 구하는 것

- 회귀분석의 검정
  - 회귀계수의 검정
    - 회귀계수 β~1~이 0이면 입력변수 X와 출력변수 y 사이에는 아무런 인과관계가 없음
    - 회귀계수 β~1~이 0이면 적합된 추정식은 아무 의미가 없음 (귀무가설 β~1~=0, 대립가설 ~1~≠0)

### 3. 다중선형회귀분석
- 다중선형회귀분석(다변량회귀분석)
  - 다중회귀식
    - Y = β~0~ + β~1~X~1~ + β~2~X~2~ + ... + β~k~X~k~ + ε  
  
  - 모형의 통계적 유의성
    - 모형의 통계적 유의성은 F통계량으로 확인
    - 유의수준 5% 하에서 F통계량의 p-값이 0.05보다 작으면 추정된 회귀식은 통계적으로 유의하다 볼 수 있음
    - F통계량이 크면 p-value가 0.05보다 작아지고 귀무가설을 기각함 → 모형이 유의하다고 결론 내릴 수 있음  
  
  - 회귀계수의 유의성
    - 단변량 회귀분석의 회귀계수 유의성 검토와 같이 t통계량을 통해 확인
    - 모든 회귀계수의 유의성이 통계적으로 검증되어야 선택된 변수 조합으로 모형 확인 가능  
  
  - 모형의 설명력
    - 결정계수(R^2^)나 수정된 결정계수(R^2^~α~) 확인  
    
  - 모형의 적합성
    - 잔차와 종속변수의 산점도로 모형이 데이터를 잘 적합하고 있는지 확인
  
  - 데이터가 전제하는 가정을 만족하는가?
    - 선형성, 독립성, 등분산성, 비상관성, 정상성
  
  - 다중공선성(multicollinearity)
    - 다중회귀분석에서 설명변수 사이에 선형관계가 존재하면 회귀계수의 정확한 추정이 곤란함
    - 다중공선성 검사 방법
      - 분산팽창요인(VIF): 4보다 크면 다중공산성 존재한다고 볼 수 있고, 10보다 크면 심각한 문제가 있다고 해석
      - 상태지수: 10 이상이면 문제 있다고 보고, 30보다 크면 심각한 문제가 있다고 해석
        - 다중선형회귀분석에서 다중공선성 문제 발생 시, 문제 있는 변수를 제거하거나 주성분회귀, 능형회귀 모형을 적용하여 문제 해결

### 4. 회귀분석의 종류

| 종류 | 내용 |
|---|---|
| 단순회귀 | 독립변수가 1개이며 종속변수와의 관계가 직선 |
| 다중회귀 | 독립변수가 k개이며 종속변수와의 관계가 선형(1차 함수) |
| 로지스틱 회귀 | 종속변수가 범주형(2진변수)인 경우에 적용, 단순 로지스틱 회귀 및 다중, 다항 로지스틱 회귀로 확장할 수 있음 |
| 다항회귀 | 독립변수와 종속변수와의 관계가 1차 함수 이상인 관계(단, k=1이면 2차 함수 이상) |
| 곡선회귀 | 독립변수가 1개이며 종속변수와의 관계가 곡선 |
| 비선형회귀 | 회귀식 모양이 미지의 모수들의 선형관계로 이뤄져 있지 않은 모형 |

### 5. 회귀분석 사례
- F-statistic: F-통계량
- p-value: 유의수준 5% 하에서 추정되어야 해당 회귀 모형이 통계적으로 유의하다고 할 수 있음
- Multiple R-squared: 결정계수, Adjusted R-squared: 수정된 결정계수 (0~1값을 가지며, 높을수록 회귀식의 설명력 높아짐)
- Pr: 회귀계수들의 p-값

### 6. 최적회귀방정식
- 설명변수 선택
  - 상황에 따라 필요한 변수만 선택
  - y에 영향을 미칠 수 있는 모든 설명변수 x가 y값 예측에 참여
  - 데이터에 설명변수 x 수가 많아지면 관리가 어려워, 가능한 범위 내에서 적은 수의 설명변수만 포함

- 모형선택(exploratiry analysis)
  - 분석 데이터에 가장 잘 맞는 모형을 찾는 방법
  - 가능한 모든 조합의 회귀분석(All possible regression): 가능한 모든 독립변수 조합에 대한 회귀모형을 생성한 뒤, 가장 적합한 회귀모형 선택
  
- 단계적 변수 선택(Stepwise Variable Selection)
  - **전진선택법**(forward selection)
    - 절편만 있는 상수모형으로 시작해 중요하다고 생각되는 설명변수부터 모형에 추가
  
  - **후진제거법**(backward selection)
    - 독립변수 후보 모두를 포함한 모형에서 출발해 가장 적은 영향을 주는 변수부터 제거
    - 더 제거할 변수가 없을 때의 모형을 선택
  
  - **단계선택법**(stepwise selection)
    - 전진선택법에 이해 변수를 추가하며, 새롭게 추가된 변수에 기인해 기존 변수 중요도가 약화되면 해당변수를 제거
    - 단계별로 추가 또는 제거되는 변수 여부를 검토하고 더 이상 없을 때 중단

- 벌점화된 선택기준
  - 모형 복잡도에 벌점을 주는 방법
    - **AIC**(Akaike information criterion)
    - **BIC**(Bayesian information criterion)
    
  - 모든 후보 모형에 대해 AIC 또는 BIC를 계산하고 값이 최소가 되는 모형을 선택
  - 모형 선택의 일치성(consistency inselection)
    - 자료 수가 늘어날 때 참인 모형이 주어진 모형 선택 기준의 최소값을 갖게 되는 성질
    - 이론적으로 AIC에 대해 일치성이 성립하지 않지만, BIC는 주요 분포에서 이러한 성질이 성립
  - AIC 활용이 보편화된 방식
  - 추가: RIC(Risk inflation criterion), CIC(Covariance inflation criterion), DIC(Deviation information criterion)

- 최적회귀방정식 사례: 교재 참고
  1. 변수 선택법 예제(유의확률 기반)
  2. 변수 선택법 예제(벌점화 전진선택법)
  3. 변수 선택법 예제(벌점화 후진제거법)
  

## 4절. 시계열 분석
### 1. 시계열 자료
- 시계열 자료
  - 시간의 흐름에 따라 관찰된 값
  - 시계열 데이터 분석을 통해 미래의 값을 예측하고 경향, 주기, 계절성 등을 파악하여 활용

- 시계열 자료의 종류
  1. 비정상성 시계열 자료
     - 시계열 분석을 실시할 때, 다루기 어려운 자료
  
  2. 정상성 시계열 자료
     - 비정상 시계열을 핸들링해 다루기 쉬운 시계열 자료로 변환한 자료

### 2. 정상성
- 평균이 일정할 경우
  - 모든 시점에 대해 일정한 평균을 가짐
  - 평균이 일정하지 않은 시계열은 **차분(difference)을 통해 정상화**할 수 있음
  - 차분?
    - 현 시점 자료에서 전 시점 자료를 빼는 것
    - 일반차분: 바로 전 시점 자료를 빼는 방법, 계절차분: 여러 시점 전의 자료를 빼는 방법

- 분산이 일정
  - 분산도 시점에 의존하지 않고 일정해야 함
  - 분산이 일정하지 않을 경우 **변환(transformation)을 통해 정상화**할 수 있음

- 공분산도 단지 시차에만 의존, 실제 특정 시점 t, s에는 의존하지 않음

- 정상 시계열
  - 어떤 시점에서 평균과 분산, 특정한 시차의 길이를 갖는 자기공분산을 측정하더라도 동일한 값을 가짐
  - 정상 시계열은 항상 그 평균값으로 회귀하려는 경향이 있으며, 그 평균값 주변에서의 변동은 대체로 일정한 폭을 가짐
  - 정상 시계열이 아닌 경우 특정 기간의 시계열 자료로부터 얻은 정보를 다른 시기로 일반화할 수 없음

### 3. 시계열자료 분석방법
- 분석방법
  - 회귀분석(계량경제) 방법, Box-Jenkins 방법, 지수평활법, 시계열 분해법 등

- 자료 형태에 따른 분석방법
  - 일변량 시계열분석
    - Box-Jenkins(ARMA), 지수평활법, 시계열 분해법
    - 시간(t)을 설명변수로 한 회귀모형주가, 소매물가지수 등 하나의 변수에 관심 갖는 경우의 시계열분석
  
  - 다중 시계열분석
    - 계량경제모형, 전이함수모형, 개입분석, 상태공간분석, 다변량 ARIMA 등
    - 여러 개의 시간(t)에 따른 변수들을 활용하는 시계열 분석

- 이동평균법

- 지수평활법


### 4. 시계열모형
교재 참고


## 5절. 다차원척도법
### 1. 다차원척도법(Multidimensional Scaling)
- 객체간 근접성을 시각화하는 통계기법
- 군집분석과 같이 개체를 대상으로 변수들을 측정한 후, 개체 사이의 유사성/비유사성을 측정하여 개체들을 2차원 공간상에 점으로 표현하는 분석 방법
- 개체들을 2차원 또는 3차원 공간상에 점으로 표현하여 개체들 사이의 집단화를 시각적으로 표현하는 분석 방법

### 2. 다차원척도법 목적
- 데이터 속에 잠재해 있는 패턴, 구조를 찾아냄
- 찾아낸 구조를 소수 차원의 공간에 기하학적으로 표현
- 데이터 축소 목적으로 다차원척도법을 이용 → 데이터에 포함되는 정보를 끄집어내기 위한 탐색수단
- 다차원척도법에 의해 얻은 결과를, 데이터가 만들어진 현상이나 과정에 고유의 구조로서 의미 부여

### 3. 다차원척도법 방법
- 객체들의 거리 계산: 유클리드 거리행렬 활용
- 관측대상의 상대적 거리 정확도를 높이기 위해 적합 정도를 스트레스값으로 나타냄
- 각 개체를 공간상에 표현하기 위한 방법: 부적합도 기준으로 STRESS나 S-STRESS 사용
- 최적모형의 적합은 부적합도를 최소로 하는 반복알고리즘을 이용하며, 이 값이 일정 수준 이하가 될 때 최종적으로 적합된 모형으로 제시
- STRESS와 적합도 수준 M은 개체들을 공간상에 표현하기 위한 방법으로 STRESS나 S-STRESS를 부적합도 기준으로 사용

  | STRESS | 적합도 수준 |
  |---|---|
  | 0 | 완벽(perfect) |
  | 0.05 이내 | 매우 좋은(excellent) |
  | 0.05 ~ 0.10 | 만족(satisfactory) |
  | 0.10 ~ 0.15 | 보통(acceptable, but doubt) |
  | 0.15 이상 | 나쁨(poor) |


### 4. 다차원척도법 종류
- 계량적 MDS(Metric MDS)
  - 데이터가 구간척도나 비율척도인 경우 활용
  - N개의 케이스에 대해 P개의 특성변수가 있는 경우, 각 개체들 간 유클리드 거리행렬을 계산하고 개체들 간 비유사성 S(거리제곱 행렬의 선형함수)를 공간상에 표현

- 비계량적 MDS(nonmetric MDS)
  - 데이터가 순서척도인 경우 활용
  - 개체들 간 거리가 순서로 주어진 경우에는 순서척도를 거리의 속성과 같도록 변환하여 거리를 생성한 후 적용


## 6절. 주성분분석
### 1. 주성분분석(Principal Component Analysis)
- 여러 변수들이 변량을 주성분이라는 서로 상관성이 높은 변수의 선형 결합으로 만들어 기존 상관성이 높은 변수들을 요약, 축소하는 기법
- 첫 번째 주성분으로 전체 변동을 가장 많이 설명할 수 있도록 하고, 두 번째 주성분으로는 첫 번째 주성분과는 상관성이 없어서(낮아서) 첫 번째 주성분이 설명하지 못하는 나머지 변동을 정보의 손실 없이 가장 많이 설명할 수 있도록 변수들의 선형조합을 만듦

### 2. 주성분분석의 목적
- 여러 변수들 간 내재하는 상관관계, 연관성을 이용해 소수의 주성분으로 차원을 축소함으로써 데이터를 이해하기 쉽고 관리하기 쉽게 함
- 다중공선성이 존재하는 경우, 상관성 없는(적은) 주성분으로 변수들을 축소하여 모형 개발에 활용
- 회귀분석이나 의사결정나무 등 모형 개발 시, 입력변수들 간 상관관계가 높은 다중공선성이 존재할 경우 모형이 잘못 만들어져 문제 발생
- 연관성 높은 변수를 주성분분석을 통해 차원을 축소한 후, 군집분석을 수행하면 군집화 결과와 연산속도 개선 가능
- 기계에서 나오는 센서데이터를 주성분분석으로 차원 축소 후, 시계열로 분포나 추세 변화를 분석하면 기계의 고장 징후를 사전에 파악하는 데 활용할 수 있음

### 3. 주성분분석 vs 요인분석
- **요인분석**(Factor Analysis)
  - 등간척도(혹은 비율척도)로 측정한 두 개 이상 변수에 잠재된 공통인자를 찾아내는 기법

- 공통점
  - 모두 데이터를 축소하는 데 활용
  - 원래 데이터를 활용하여 몇 개의 새로운 변수 생성 가능

- 차이점
  - 생성된 변수의 수
    - 요인분석은 몇 개라고 지정 없이(2 or 3, 4, 5 ...) 만들 수 있음
    - 주성분분석은 제1주성분, 제2주성분, 제3주성분 정도로 활용(대략 4개 이상은 넘지 않음)
  
  - 생성된 변수 이름
    - 요인분석은 분석자가 요인 이름을 명명
    - 주성분분석은 주로 제1주성분, 제2주성분 등으로 표현
  
  - 생성된 변수 간 관계
    - 요인분석은 새 변수들은 기본적으로 대등한 관계를 가짐
    - 요인분석은 어떤 것이 더 중요하다는 의미가 없음(분류/예측의 다음 단계로 사용되면 중요성 부여)
    - 주성분분석은 제1주성분이 가장 중요, 그 다음 제2주성분이 중요
  
  - 분석 방법의 의미
    - 요인분석은 목표변수를 고려하지 않고 데이터가 주어지면 변수를 비슷한 성격으로 묶어서 새로운 (잠재)변수를 만듦
    - 주성분분석은 목표변수를 고려하여 목표변수를 예측/분류하기 위해 원래 변수의 선형 결합으로 이뤄진 몇 개의 주성분(변수)를 찾게 됨

### 4. 주성분의 선택법
- 주성분분석 결과에서 누적기여율(cumulative proportion)이 85% 이상이면 주성분 수로 결정할 수 있음
- scree plot을 활용하여 고유값(eigenvalue)이 수평을 유지하기 전 단계로 주성분의 수 선택

### 5. 주성분 분석 사례
- 교재 참고


# 5장. 정형 데이터 마이닝
## 1절. 데이터마이닝의 개요

### 1. 데이터마이닝
- 데이터마이닝
  - 대용량 데이터에서 의미 있는 패턴을 파악하거나 예측하여 의사결정에 활용하는 방법

- 통계분석과의 차이점
  - 통계분석은 가설이나 가정에 따른 분석이나 검증을 함
  - 데이터마이닝은 다양한 수리 알고리즘을 이용해 데이터베이스의 데이터로부터 의미 있는 정보를 찾아내는 방법을 통칭
  
- 종류

  | 정보를 찾는 방법론에 따라 | 분석대상, 활용목적, 표현방법에 따라 |
  |---|---|
  | 인공지능, 의사결정나무, K-평균군집화, 연관분석, 회귀분석, 로짓분석, 최근접이웃 | 시각화분석, 분류, 군집화, 포케스팅 |
  
- 사용분야
  - 병원: 환자 데이터를 이용하여 해당 환자에게 발생 가능성 높은 병 예측
  - 병원: 기존 환자가 응급실에 왔을 때, 어떤 조치를 먼저 해야 하는지 결정
  - 은행: 고객 데이터를 이용해 해당 고객의 우량/불량을 예측하여 대출 여부 판단
  - 공항: 세관 검사에서 입국자 이력과 데이터를 이용해 관세품 반입 여부 예측

### 2. 데이터마이닝의 분석 방법

| Supervised Data Prediction(지도학습) | Unsupervised Data Prediction(비지도학습) |
|---|---|
| 의사결정나무, 인공신경망, 일반화 선형 모형, 회귀분석, 로지스틱 회귀분석, 사례기반 추론, 최근접 이웃법 | OLAP, 연관성 규칙발견, 군집분석, SOM |

### 3. 분석 목적에 따른 작업 유형과 기법

- 예측(Predictive Modeling): 분류 규칙
- 설명(Descriptive Modeling): 연관 규칙, 연속 규칙, 데이터 군집화

| 작업유형 | 설명 | 사용기법 |
|---|---|---|
| 분류 규칙(Classification) | 가장 많이 사용되는 작업으로 과거 데이터로부터 고객특성을 찾아 분륨형을 만들어 이를 토대로 새로운 레코드의 결과값을 예측하는 것, 목표 마케팅 및 고객 신용평가 모형에 활용 | 회귀분석, 판별분석, 신경망, 의사결정나무 |
| 연관규칙(Association) | 데이터 안에 존재하는 항목간의 종속관계를 찾아내는 작업, 제품이나 서비스 교차판매, 매장진열, 첨부우편, 사기적발 등 분야에 활용 | 동시발생 매트릭스 |
| 연속규칙(Sequence) | 연관 규칙에 시간 관련 정보가 포함된 형태, 고객 구매이력 속성이 반드시 필요, 목표 마케팅이나 일대일 마케팅에 활용 | 동시발생 매트릭스 |
| 데이터 군집화(Clustering) | 고객 레코드를 유사한 특성을 지닌 몇 개의 소그룹으로 분할, 작업 특성이 분류규칙과 유사하나 분석대상 데이터에 결과값이 없음, 판촉활동이나 이벤트 대상 선정에 활용 | K-Means Clustering |

### 4. 데이터마이닝 추진단계
1. 목적 설정
2. 데이터 준비
3. 가공
4. 기법 적용
5. 검증

### 5. 데이터마이닝을 위한 데이터 분할
- 개요
  - 모델 평가용 테스트 데이터와 구축용 데이터로 분할
  - 구축용 데이터로 모형을 생성하고 테스트 데이터로 모형이 얼마나 적합한지를 판단

- 데이터 분할
  1. 구축용(training data, 50%)
     - 추정용, 훈련용 데이터라고도 불리며 데이터마이닝 모델을 만드는 데 활용
    
  2. 검정용(validation data, 30%)
     - 구축된 모형의 과대추정 또는 과소추정을 미세 조정하는 데 활용
    
  3. 시험용(tast data, 20%)
     - 테스트 데이터나 과거 데이터를 활용하여 모델의 성능을 검증하는 데 활용
  
  4. 데이터 양이 충분하지 않거나 입력 변수에 대한 설명이 충분한 경우
     - 홀드아웃 방법
       - 주어진 데이터를 랜덤하게 두 개의 데이터로 구분하여 사용
       - 주로 학습용과 시험용으로 분리하여 사용
     - 교차확인 방법
       - 주어진 데이터를 k개의 하부집단으로 구분
       - k-1개의 집단을 학습용으로 나머지는 하부집단으로 검증용으로 설정하여 학습
       - k번 반복 측정한 결과를 평균낸 값을 최종값으로 사용
       - 주로 10-fold 교차분석을 많이 사용

### 6. 성과분석
- 오분류에 대한 추정치
  1. **정분류율**(Accuracy)
     - Accuracy = $\frac{TN + TP}{TN + TP + FN + FP}$
  
  2. **오분류율**(Error Rate)
     - 1 - Accuracy = $\frac{FN + FP}{TN + TP + FN + FP}$
     
  3. **특이도**(Specificity)
     - Specificity = $\frac{TN}{TN + FP}$ (TNR: True Negative Rate)
  
  4. **민감도**(Sensitivity)
     - Sensitivity = $\frac{TP}{TP + FN}$ (TPR: True Positive Rate)
  
  5. **정확도**(Precision)
     - Precision = $\frac{TP}{TP + FP}$
  
  6. **재현율**(Recall): 민감도와 같음
     - Recall = $\frac{TP}{TP + FN}$
  
  7. **F1 Score**
     - F1 = 2 x $\frac{Precision x Recall}{Precision + Recall}$

- ROCR 패키지로 성과분석
  - ROC Curve(Receiver Operation Characteristic Curve)
    - 가로축을 FPR(False Positive Rate = 1 - 특이도)값, 세로축을 TPR(Ture Positive Rate, 민감도)값으로 두어 시각화한 그래프
    - 2진 분류(binary classfication)에서 모형 성능을 평가하기 위해 사용되는 척도
    - 그래프가 왼쪽 상단에 가깝게 그려질수록 올바르게 예측한 비율은 높고 잘못 예측한 비율은 낮음을 의미
    - **ROC 곡선 아래 면적을 의미하는 AUROC(Area Under ROC) 값이 클수록(1에 가까울 수록) 모형 성능이 좋다고 평가**

    - TPR: 1인 케이스에 대한 1로 예측한 비율
    - FPR: 0인 케이스에 대한 1로 잘못 예측한 비율
    - AUROC를 이용한 정확도의 판단 기준
    
      | 기준 | 구분 |
      |---|---|
      | 0.9 - 1.0 | excellent (A) |
      | 0.8 - 0.9 | good |
      | 0.7 - 0.8 | fair |
      | 0.6 - 0.7 | poor |
      | 0.5 - 0.6 | fail |

- 이익도표(Lift chart)
  - 분류모형 성능을 평가하기 위한 척도 (분류된 관측치에 대해 예측이 얼마나 잘 이루어졌는지)
  - 임의로 나눈 등급별로 반응검출율, 반응률, 리프트 등 정보를 산출해 나타내는 도표
  - 기본 향상도에 비해 반응률이 몇 배나 높은지 계산: 향상도(Lift)
  - 각 등급은 예측확률에 따라 매겨진 순위이므로, 상위 등급에서는 더 높은 반응률을 보이는 것이 좋은 모형

## 2절. 분류분석
### 1. 분류분석과 예측분석
- 분류분석의 정의
  - 데이터가 어떤 그룹에 속하는지 예측할 때 사용하는 기법
  - 클러스터링과 유사하나, 분류분석은 각 그룹이 정의되어 있음
  - 교사학습(supervised learning)에 해당하는 예측기법

- 예측분석의 정의
  - 시계열분석처럼 시간에 따른 값 두 개만을 이용해 앞으로의 매출 또는 온도 등을 예측하는 것
  - 모델링을 하는 입력 데이터가 어떤 것인지에 따라 특성이 다름
  - 여러 개의 다양한 설명변수(독립변수)가 아닌 한 개의 설명변수로 생각하면 됨

- 분류분석 vs 예측분석
  - 공통점
    - 레코드 특정 속성의 값을 미리 알아맞힐 수 있음
  
  - 차이점
    - 분류: 레코드(튜플)의 **범주형 속성**의 값을 맞힘(국/영/수 점수로 내신 등급 맞히기)
    - 예측: 레코드(튜플)의 **연속형 속성**의 값을 맞힘(카드 회원 가입정보로 연 매출액 알아맞히기)

- 분류 모델링
  - 신용평가모형(우량, 불량)
  - 사기방지모형(사기, 정상)
  - 이탈모형(이탈, 유지)
  - 고객세분화(VVIP, VIP, GOLD, SILVER, BRONZE)

- 분류 기법
  - 회귀분석, 로지스틱 회귀분석
  - 의사결정나무, CART, C5.0
  - 베이지안 분류
  - 인공신경망
  - 지지도벡터기계
  - k 최근접 이웃
  - 규칙기반의 분류와 사례기반추록
  
### 2. 로지스틱 회귀분석(Logistic Regression)
- **반응변수가 범주형인 경우**에 적용되는 회귀분석모형
- 새로운 설명변수(또는 예측변수)가 주어질 때, 반응변수의 각 범주(또는 집단)에 속할 확률이 얼마인지 추정(예측모형)하여, 추정 확률을 기준치에 따라 분류하는 목적(분류모형)
- **사후확률**(Posterior Probability): 모형의 적합을 통해 추정된 확률
- exp(β~1~): 나머지 변수가 주어질 때, x~1~이 한 단위 증가할 때마다 성공(Y=1)의 오즈가 몇 배 증가하는지 나타내는 값
- 표준 로지스틱 분포의 누적함수로 성공 확률을 추정

- 선형회귀분석 vs 로지스틱 회귀분석

  | 목적 | 선형회귀분석 | 로지스틱 회귀분석 |
  |---|---|---|
  | 종속변수 | 연속형 변수 | (0, 1) |
  | 계수 추정법 | 최소제곱법 | 최대우도추정법 |
  | 모형 검정 | F-검정, T-검정 | 카이제곱 검정(x^2^-test) |

- glm() 함수를 활용하여 로지스틱 회귀분석을 실행

### 3. 의사결정나무
- 정의
  - 분류함수를 의사결정 규칙으로 이뤄진 나무 모양으로 그리는 방법
  - 연속적으로 발생하는 의사결정 문제를 시각화
  - 계산결과가 의사결정나무에 직접적으로 나타나서 해석이 간편함
  - 주어진 입력값에 대해 출력값을 예측하는 모형 → 분류나무와 회귀나무 모형

- 예측력과 해석력

- 의사결정나무의 활용
  1. 세분화
     - 데이터를 비슷한 특성을 갖는 몇 개 그룹으로 분할해 그룹별 특성을 발견하는 것
  
  2. 분류
     - 여러 예측변수에 근거해 관측개체의 목표변수 범주를 몇 개 등급으로 분류하고자 하는 경우에 사용
  
  3. 예측
     - 자료에서 규칙을 찾고 이를 이용해 미래 사건을 예측하고자 하는 경우에 사용
  
  4. 차원축소 및 변수선택
     - 많은 예측변수 중 목표변수에 큰 영향을 미치는 변수를 골라내고자 하는 경우에 사용
  
  5. 교호작용효과의 파악
     - 여러 개 예측변수를 결합해 목표변수에 작용하는 규칙을 파악하고자 하는 경우
     - 범주의 병합 또는 연속형 변수의 이산화: 범주형 목표변수의 범주를 소수 몇 개로 병합하거나, 연속형 목표변수를 몇 개의 등급으로 이산화하고자 하는 경우

- **의사결정나무 특징**
  - 장점
    - 결과 설명 용이
    - 모형 만들기가 계산적으로 복잡하지 않음
    - 대용량 데이터에서도 빠르게 만들 수 있음
    - 비정상 잡음 데이터도 민감함 없이 분류 가능
    - 한 변수와 상관성 높은, 다른 불필요한 변수가 있어도 크게 영향 받지 않음
    - 설명변수나 목표변수에 수치형변수와 범주형변수 모두 사용 가능
    - 모형 분류 정확도가 높음
  
  - 단점
    - 새로운 자료에 대한 과대적합이 발생할 가능성이 높음
    - 분류 경계선 부근 자료값에 대해 오차가 큼
    - 설명변수 간 중요도 판단이 어려움

- 의사결정나무 분석 과정
  - 성장: 적절한 정지규칙을 만족하면 중단
  - 가지치기
  - 타당성 평가
  - 해석 및 예측

- 나무의 성장
  - 분리규칙(splitting rule)
  - 분리기준(splitting criterion)
   - 이산형 목표변수
    
      | 기준값 | 분리기준 |
      |---|---|
      | 카이제곱 통계량 p값 | P값이 가장 작은 예측변수와 그때의 최적분리에 의해 자식마디 형성 |
      | 지니지수 | 지니지수를 감소시키는 예측변수와 그때의 최적분리에 의해 자식마디 선택 |
      | 엔트로피 지수 | 엔트로피 지수가 가장 작은 예측변수와 그때의 최적분리에 의해 자식마디 형성 |
    
   - 연속형 목표변수
      
      | 기준값 | 분리기준 |
      |---|---|
      | 분산분석에서 F통계량 | P값이 가장 작은 예측변수와 그때의 최적분리에 의해 자식마디 형성 |
      | 분산의 감소량 | 분산 감소량을 최대화하는 기준의 최적분리에 의해 자식마디 형성 |
  
  - 정지규칙
    - 더이상 분리가 일어나지 않고 현재 마디가 끝마디가 되도록 하는 규칙
    - 정지기준: 의사결정나무 깊이를 지정, 끝마디의 레코드 수의 최소 개수를 지정

- **나무의 가지치기**(Pruning)
  - 너무 큰 나무모형은 자료를 과대적합, 너무 작은 나무모형은 과소적합할 위험
  - 나무 크기를 모형 복잡도로 볼 수 있으며, 최적 나무 크기는 자료로부터 추정하게 됨
  - 일반적으로 사용되는 방법은 마디에 속하는 자료가 일정 수(가령 5) 이하일 때 분할을 정지
  - 비용 - 복잡도 가지치기를 이용하여 성장시킨 나무를 가지치기하게 됨

### 4. 불순도의 여러 가지 측도
- 목표변수가 범주형 변수인 의사결정나무 분류규칙을 선택
  1. 카이제곱 통계량
     - 각 셀에 대한 ((실제도수 - 기대도수)의 제곱 / 기대도수) 합으로 구할 수 있음
     - 기대도수 = 열의 합계 x 합의 합계 / 전체합계
  
  2. 지니지수
     - 노드의 불순도를 나타내는 값
     - 지니지수 값이 클수록 이질적이며 순수도가 낮다고 볼 수 있음
  
  3. 엔트로피 지수
     - 열역학에서 쓰는 개념으로 무질서 정도에 대한 측도
     - 엔트로피 지수 값이 클수록 순수도가 낮다고 볼 수 있음
     - 엔트로피 지수가 가장 작은 예측변수와 이때의 최적분리 규칙에 의해 자식마디 형성

### 5. 의사결정나무 알고리즘
- CART
  - 불순도의 측도로 출력변수가 범주형일 경우 지니지수를 이용, 연속형인 경우 분산을 이용한 이진분리 사용
  - 개별 입력변수뿐 아니라 입력변수의 선형결합 중에서 최적의 분리를 찾을 수 있음

- C4.5와 C5.0
  - CART와는 다르게 각 마디에서 다지분리가 가능
  - 범주형 입력변수에 대하여는 범주 수만큼 분리가 일어남
  - 불순도의 측도로는 엔트로피지수 사용

- CHAID
  - 가지치기 하지 않고 적당한 크기에서 나무모형의 성장을 중지
  - **입력변수가 반드시 범주형 변수여야 함**
  - 불순도 측도로는 카이제곱 통계량 사용

## 3절. 앙상블 분석
- 앙상블
  - 주어진 자료로부터 여러 개 예측모형을 만든 후, 조합하여 하나의 최종 예측 모형을 만드는 방법
  - 다중 모델 조합, 분류기 조합

- 학습방법의 불안정성
  - 학습자료의 작은 변화에 의해 예측모형이 크게 변하는 경우, 그 학습방법은 불안정함
  - 가장 안정적인 방법
    - 1-nearest neighbor: 가장 가까운 자료만 변하지 않으면 예측모형 변하지 않음
    - 선형회귀모형: 최소제곱법으로 추정해 모형 결정
  - 가장 불안정한 방법: 의사결정나무

- 앙상블 기법의 종류
  1. **배깅**
     - 주어진 자료에서 여러 개의 붓스트랩 자료를 생성하고 각 붓스트랩 자료에 예측모형을 만든 후 결합하여 최종 예측모형을 만드는 방법
     - 붓스트랩(bootstrap): 주어진 자료에서 동일한 크기 표본을 랜덤 복원추출로 뽑은 자료
     - 보팅(voting): 여러 개 모형으로부터 산출된 결과를 다수결에 의해 최종 결과를 선정하는 과정
     - 배깅에서는 가지치기를 하지 않고 최대로 성장한 의사결정나무를 활용
     - 훈련자료 모집단의 분포를 모르기 때문에 실제 문제에서는 평균예측모형을 구할 수 없음 → 훈련자료를 모집단으로 생각하고 평균예측모형을 구하여 분산을 줄이고 예측력을 향상시킬 수 있음
  
  2. **부스팅**
     - 예측력 약한 모형을 결합하여 강한 예측모형을 만드는 방법
     - 훈련오차를 빠르고 쉽게 줄일 수 있음
     - 배깅에 비해 많은 경우의 예측오차가 향상
     - Adaboost: 이진분류 문제에서 랜덤 분류기보다 조금 더 좋은 분류기 n개에 가중치를 설정하고 n개 분류기를 결합하여 최종 분류기 만드는 방법(단, 가중치 합은 1)
  
  3. **랜덤 포레스트**(random forest)
     - 분산이 크다는 의사결정나무 특징을 고려하여 배깅과 부스팅보다 더 많은 무작위성을 줌
     - 약한 학습기를 생성한 후, 이를 선형 결합하여 최종 학습기를 만드는 방법
     - 랜덤한 forest에는 많은 트리가 생성됨
     - 정확도 측면에서 좋은 성과
     - 이론적 설명이나 최종 결과 해석이 어렵지만, 예측력이 매우 높음

## 4절. 인공신경망 분석
### 1. 인공신경망 분석(ANN)
- 인공신경망이란?
  - 인간 뇌를 기반으로 한 추론 모델
  - 뉴런: 기본적인 정보처리 단위

- 인간의 뇌를 형상화한 인공신경망
  - 인간 뇌의 특징
    - 100억 개 뉴런과 6조 개 시냅스의 결합체
    - 인간의 뇌: 컴퓨터보다 빠르고, 복잡하고, 비선형적, 병렬적인 정보 처리 시스템
    - 적응성에 따라 잘못된 답에 대한 뉴런 사이 연결은 약화되고, 올바른 답에 대한 연결이 강화됨
  
  - 인간 뇌 모델링
    - 뉴런은 가중치 있는 링크로 연결되어 있음
    - 뉴런은 여러 입력 신호를 받으나, 출력 신호는 하나만 생성함
  
- 인공신경망의 학습
  - 신경망은 **가중치를 반복적으로 조정하며 학습**
  - 뉴런은 링크로 연결되어 있고, 각 링크에는 수치적인 가중치가 있음
  - 신경망 가중치를 초기화 → 훈련 데이터로 가중치 갱신 → 신경망 구조 선택 → 활용할 학습 알고리즘 결정 → 신경망 훈련
  
- 인공신경망 특징
  1. 구조
     - 입력 링크에서 여러 신호를 받아 새로운 활성화 수준을 계산하고 출력 링크로 출력 신호를 보냄
     - 입력 신호는 미가공 데이터 또는 다른 뉴런의 출력이 될 수 있음
     - 출력 신호는 문제의 최종적인 해(solution)가 되거나 다른 뉴런에 입력될 수 있음
  
  2. 뉴런의 계산
     - 뉴런은 전이함수, 즉 활성화 함수를 사용
     - 활성화 함수를 이용해 출력을 결정하며, 입력신호의 가중치 합을 계산하여 임계값과 비교
     - 가중치 합이 임계값보다 작으면 뉴련의 출력은 -1, 같거나 크면 +1을 출력
  
  3. 뉴런의 활성화 함수
     - 시그모이드 함수: 로지스틱 회귀분석과 유사하며 0~1의 확률값을 가짐
     - softmax 함수: 표준화지수 함수로도 불리며, 출력값이 여러 개로 주어지고 목표치가 다범주인 경우 각 범주에 속할 사후확률을 제공하는 함수
     - relu 함수: 입력값이 0 이하는 0, 0 이상은 x값을 가지는 함수, 최근 딥러닝에서 많이 활용
  
  4. 단일 뉴런의 학습(단층 퍼셉트론)
     - 퍼셉트론은 선형 결합기와 하드 리미터로 구성
     - 초평면(hyperplane)은 n차원 공간을 두 개의 영역으로 나눔
     - 초평면을 선형 분리 함수로 정의

- 신경망 모형 구축 시 고려사항
  1. 입력변수
     - 신경망 모형은 복잡성으로 인해 입력 자료 선택에 매우 민감
     - 입력변수가 범주형 또는 연속형 변수일 때 아래 조건이 신경망 모형에 적합
       - **범주형 변수**: 모든 범주에서 일정 빈도 이상의 값을 갖고 각 범주 빈도가 일정할 때
       - **연속형 변수**: 입력변수 값의 범위가 변수간의 큰 차이가 없을 때
     
     - 연속형 변수의 경우, 분포가 평균을 중심으로 대칭이 아니면 좋지 않은 결과를 도출하므로 아래 방법을 활용
       - **변환**: 고객 소득(대부분 평균 미만, 특정 고객 소득이 매우 큰) 로그 변환
       - **범주화**: 각 범주 빈도가 비슷해지도록 설정
     
     - 범주형 변수의 경우 가변수화하여 적용
     - 가능한 경우 모든 범주형 변수는 같은 범위를 갖도록 가변수화 하는 것이 좋음
    
  2. 가중치의 초기값과 다중 최소값 문제
     - 역전파 알고리즘은 초기값에 따라 결과가 많이 달라짐 → 초기값 선택은 매우 중요한 문제
     - 가중치가 0이면 시그모이드 함수는 선형, 신경망 모형은 근사적으로 선형모형이 됨
     - 일반적으로 초기값은 0 근처로 랜덤하게 선택 → 초기 모형은 선형모형에 가깝고, 가중치 값이 증가할수록 비선형모형이 됨
     - 참고: 초기값이 0이면 반복해도 값이 전혀 변하지 않고, 너무 크면 좋지 않은 해를 주는 문제점 내포
  
  3. 학습모드
     - 온라인 학습모드(online learning mode)
       - 각 관측값을 순차적으로 하나씩 신경망에 투입하여 가중치 추정값이 매번 바뀜
       - 일반적으로 속도가 빠름, 훈련자료에 유사값 많은 경우 그 차이가 더 두드러짐
       - 훈련자료가 비정상성과 같이 특이한 성질을 가진 경우가 좋음
       - 국소최솟갑셍서 벗어나기 더 쉬움
    
     - 확률적 학습모드(probabilistic learning mode)
       - 온라인 학습모드와 같으나, 신경망에 투입되는 관측값의 순서가 랜덤
      
     - 배치 학습모드(batch learning mode)
       - 전체 훈련자료를 동시에 신경망에 투입
     
     - 은닉층(hidden layer)과 은닉노드(hidden node)의 수
       - 신경망을 적용할 때, 가장 중요한 부분이 모형의 선택
       - 은닉층과 은닉노드가 많으면 가중치가 많아져서 과대 적합 문제 발생
       - 은닉층과 은닉노드가 적으면 과소적합 문제 발생
       - 은닉층 수가 하나인 신경망: 범용 근사자 → 모든 매끄러운 함수 근사적 표현 가능
       - 은닉노드 수는 적절히 큰 값으로 놓고 가중치를 감소시키며 적용하는 것이 좋음
    
     - 과대 적합 문제
       - 신경망에서는 많은 가중치를 추정해야 하므로 과대적합 문제가 빈번히 발생
       - 알고리즘 조기종료와 가중치 감소 기법으로 해결할 수 있음
       - 모형 적합 과정에서 검증오차가 증가하면 반복을 중지하는 조기종료 시행
       - 선형모형의 능형회귀와 유사한 가중치 감소라는 벌점화 기법 활용

## 5절. 군집분석
### 1. 군집분석
- 개요
  - 각 객체(대상)의 유사성을 측정하여 유사성이 높은 대상 집단을 분류하고, 군집에 속한 객체들의 유사성과 서로 다른 군집에 속한 객체간 상이성을 규명하는 분석 방법
  - 특성에 따라 고객을 여러 개의 배타적인 집단으로 나눔
  - 결과는 구체적인 군집분석 방법에 따라 차이 날 수 있음
  - 군집 개수나 구조에 관한 가정 없이 데이터 사이 거리를 기준으로 군집화 유도
  - 마케팅 조사에서 소비자의 상품구매행동이나 life style에 따른 소비자군을 분류하여 시장 전략 수립에 활용

- 특징
  - 요인분석과의 차이
    - 요인분석은 유사한 변수를 함께 묶는 목적
  
  - 판별분석과의 차이
    - 판별분석은 사전에 집단이 나뉜 자료를 통해 새로운 데이터를 기존 집단에 할당하는 것이 목적

### 2. 거리
- 군집분석에서는 관측 데이터 간 유사성이나 근접성을 측정해 어느 군집으로 묶을 수 있는지 판단해야 함  
- 아래 각 거리 식은 교재 참고

- **연속형 변수의 경우**
  - 유클리디안 거리
    - 데이터 유사성 측정할 때 많이 사용하는 거리, 통계적 개념 내포 x → 변수 산포 정도가 감안되지 않음
  
  - 표준화 거리
    - 해당변수 표준편차로 척도 변환 후, 유클리드안 거리를 계산하는 방법
    - 표준화하게 되면 척도 차이, 분산 차이로 인한 왜곡을 피할 수 있음
  
  - 마할라노비스 거리
    - 통계적 개념이 포함된 거리이며 변수들의 산포를 고려하여 이를 표준화한 거리
    - 두 백터 사이 거리를 산포를 의미하는 표본공분산으로 나눠주어야 함
    - 그룹에 관한 사전 지식 없이는 표본공분산S를 계산할 수 없으므로 사용하기 곤란
  
  - 체비셰프 거리
  
  - 맨하탄 거리
    - 유클리디안 거리와 함께 가장 많이 사용되는 거리
    - 맨하탄 도시 건물에서 건물을 가기 위한 최단 거리를 구하기 위해 고안
  
  - 캔버라 거리
  
  - 민코우스키 거리
    - 맨하탄 거리와 유클리디안 거리를 한 번에 표현한 공식
    - L1 거리(맨하탄거리), L2 거리(유클리디안 거리)라고 불림

- **범주형 변수의 경우**
  - 자카드 거리
  - 자카드 계수
  - 코사인 거리
    - 유사도 기준으로 문서를 분류/그룹핑할 때 유용하게 사용
  - 코사인 유사도
    - 두 개체 백터 내적의 코사인 값을 이용하여 측정된 백터간의 유사한 정도

### 3. 계층적 군집분석
- n개의 군집으로 시작해 점차 군집 개수를 줄여가는 방법
- 계층적 군집을 형성하는 방법에는 합병형 방법과 분리형 방법이 있음

1. 최단연결법(single linkage, nearest neighbor)
   - n*n 거리행렬에서 거리가 가장 가까운 데이터를 묶어서 군집 형성
   - 군집과 군집 또는 데이터와의 거리 계산 시, 최단거리(min)를 거리로 계산하여 거리행렬 수정 진행
   - 수정된 거리행렬에서 거리가 가까운 데이터/군집을 새로운 군집으로 형성

2. 최장연결법(complete linkage, farthest neighbor)
   - 군집과 군집/데이터와 거리 계산 시, 최장거리(max)를 거리로 계산하여 거리행렬을 수정하는 방법
  
3. 평균연결법(average linkage)
   - 군집과 군집/데이터와 거리 계산 시, 평균(mean)을 거리로 계산하여 거리행렬을 수정하는 방법
  
4. 와드연결법(ward linkage)
   - 군집 내 편차들의 제곱합을 고려한 방법
   - 군집간 정보 손실 최소화를 위해 군집화 진행

5. 군집화
   - 거리행렬을 통해 가장 가까운 거리의 객체들간 관계를 규명하고 덴드로그램을 그림
   - 덴드로그램을 보고 군집 개수를 변화해가며 적절한 군집 수 선정
   - 군집 수는 분석 목적에 따라 선정할 수 있지만, 5개 이상은 잘 활용하지 않음
   - 군집화 단계
     - 거리행렬을 기준으로 덴드로그램을 그림
     - 덴드로그램 최상단부터 세로축 개수에 따라 가로선을 그어 군집 개수 선택
     - 각 객체 구성을 고려하여 적절한 군집 수 선정

### 4. 비계층적 군집분석
n개의 개체를 g개의 군집으로 나눌 수 있는 모든 가능한 방법을 점검해 최적화한 군집을 형성하는 것

- K-평균 군집분석의 개념
  - 주어진 데이터를 k개의 클러스터로 묶는 알고리즘
  - 각 클러스터와 거리 차이 분산을 최소화하는 방식으로 동작

- K-평균 군집분석 과정
  - 원하는 군집 개수와 초기 값(seed)을 정해 seed 중심으로 군집 형성
  - 각 데이터를 거리가 가장 가까운 seed가 있는 군집으로 분류
  - 각 군집의 seed 값을 다시 계산
  - 모든 개체가 군집으로 할당될 때까지 위 과정 반복

- K-평균 군집분석 특징
  - 거리 계산을 통해 군집화가 이루어지므로 **연속형 변수에 활용 가능**
  - K개의 **초기 중심값은 임의 선택 가능**하며 가급적이면 멀리 떨어지는 것이 바람직함
  - 초기 중심값을 임의로 선택할 때, 일렬로 선택하면 군집이 혼합되지 않고 층으로 나눠질 수 있어 주의해야 함
  - **초기 중심값 선정에 따라 결과가 달라질 수 있음**
  - 초기 중심으로부터 오차 제곱합을 최소화하는 방향으로 군집이 형성되는 **탐욕적(greedy) 알고리즘**이므로 안정된 군집은 보장하나 최적이라는 보장은 없음
  
  | 장점 | 단점 |
  |---|---|
  | 알고리즘 단순, 수행 빠름 → 분석 방법 적용 용이, 계층적 군집분석에 비해 많은 양의 데이터를 다룰 수 있음, 내부 구조에 대한 사전정보 없이 의미 있는 자료구조 찾을 수 있음, 다양한 형태 데이터에 적용 가능 | 군집 수, 가중치와 거리 정의가 어려움, 사전에 주어진 목적이 없으므로 결과 해석이 어려움, 잡음이나 이상값의 영향을 많이 받음, 볼록한 형태가 아닌 군집이 존재할 경우에는 성능 떨어짐, 초기 군집 수 결정이 어려움 |

### 5. 혼합 분포 군집(mixture distribution clustering)
- 개요
  - 모형 기반 군집 방법
  - 데이터가 k개의 모수적 모형의 가중합으로 표현되는 모집단 모형으로부터 나왔다는 가정 하에서 모수와 함께 가중치를 자료로부터 추정하는 방법 사용
  - K개의 각 모형은 군집을 의미, 각 데이터는 추정된 k개의 모형 중 어느 모형으로부터 나왔을 확률이 높은지에 따라 군집 분류가 이루어짐
  - 혼합모형에서 모수와 가중치의 추정(최대가능도 추정)에는 EM 알고리즘이 사용됨

- 혼합 분포모형으로 설명할 수 있는 데이터 형태
  - 자료의 분포형태가 다봉형의 형태
  - 교재 참고

- EM(Expectation-Maximization) 알고리즘의 진행 과정
  - 각 자료에 대해 Z의 조건부분포(어느 집단에 속할지에 관한)로부터 조건부 기댓값을 구할 수 있음
  - 관측변수 X와 잠재변수 Z를 포함하는 (X,Z)에 대한 로그-가능도함수에 Z 대신 상수값인 Z의 조건부 기댓값을 대입하면 로그-가능도함수를 최대로 하는 모수를 쉽게 찾을 수 있음, (M-단계) 갱신된 모수 추정치에 위 과정을 반복하면 수렴하는 값을 얻게 되고 이는 최대 가능도 추정치로 사용될 수 있음
  - E-단계: 잠재변수 Z의 기대치 계산
  - M-단계: 잠재변수 Z의 기대치를 이용하여 파라미터 추정

- 혼합 분포 군집모형의 트깅
  - K-평균군집 절차와 유사하지만 **확률분포를 도입하여 군집 수행**
  - 군집을 몇 개의 모수로 표현할 수 있으며, 서로 다른 크기나 모양의 군집을 찾을 수 있음
  - EM 알고리즘을 이용한 모수 추정에서 데이터가 커지면 수렴에 시간이 걸릴 수 있음
  - 군집 크기가 너무 작으면 추정 정도가 떨어지거나 어려울 수 있음
  - K-평균군집과 같이 **이상치 자료에 민감** → 사전 조치 필요

### 6. SOM(Self Organizing Map)
- SOM
  - 자가조직화지도 알고리즘은 코호넨에 의해 제시, 개발 → 코호넨 맵(Ko-honen Maps)이라고도 알려져 있음
  - **SOM은 비지도 신경망으로 고차원의 데이터를 이해하기 쉬운 저차원의 뉴런으로 정렬**하여 지도 형태로 형상화
  - 형상화는 입력 변수의 위치 관계를 그대로 보존한다는 특징이 있음 → 실제 공간의 입력변수가 가까이 있으면 지도상에도 가까운 위치에 있음

- 구성
  - SOM 모델은 두 개의 인공신경망 층으로 구성되어 있음
  1. **입력층(Input layer, 입력벡터를 받는 층)**
     - 입력변수 개수와 뉴런 수가 동일하게 존재
     - 입력층 자료는 학습을 통해 경쟁층에 정렬되는데, 이를 지도라 부름
     - 입력층에 있는 각각의 뉴런은 경쟁층에 있는 각각의 뉴런과 완전 연결(fully connected)되어 있음
  
  2. **경쟁층(Competitive layer, 2차원 격자(grid)로 구성된 층)**
     - 입력백터 특성에 따라 백터가 한 점으로 클러스터링 되는 층
     - SOM은 경쟁 학습으로 각각 뉴런이 입력백터와 얼마나 가까운가를 계산하여 연결 강도(connection weight)를 반복적으로 재조정하여 학습
     - 위 과정을 거치며 연결강도는 입력 패턴과 가장 유사한 경쟁층 뉴런이 승자가 됨
     - 입력층 표본 백터에 가장 가까운 프로토타입 백터를 BMU(Best-Matching-Unit)라고 하며, 코호넨 승자 독점의 학습 규칙에 따라 위상학적 이웃(topological neighbors)에 대한 연결 강도를 조정
     - 승자 독식 구조로 인해 경쟁층에는 승자 뉴런만이 나타나며, 승자와 유사한 연결 강도를 갖는 입력 패턴이 동일한 경쟁 뉴런으로 배열됨

- 특징
  - 고차원의 데이터를 저차원의 지도 형태로 형상화 → 시각적으로 이해가 쉬움
  - 입력변수의 위치 관계를 그대로 보존하기 때문에 실제 데이터가 유사하면 지도상에서 가깝게 표현 → 패턴 발견, 이미지 분석 등에서 뛰어난 성능을 보임
  - 역전파(Back Propagation): 알고리즘 등을 이용하는 인공신경망과 달리 단 하나의 전방 패스를 사용함으로써 속도가 매우 빠름 → 실시간 학습처리를 할 수 있는 모형

- SOM과 신경망 모형의 차이점

| 구분 | 신경망 모형 | SOM |
|---|---|---|
| 학습 방법 | 오차역전파법 | 경쟁학습방법 |
| 구성 | 입력층, 은닉층, 출력층 | 입력층, 2차원 격자 형태의 경쟁층 |
| 기계 학습 방법의 분류 | 지도학습(Supervised Learning) | 비지도학습(Unsupervised Learning) |

###7. 최신 군집분석 기법
- 교재 참고


## 6절. 연관분석
### 1. 연관규칙
- 연관규칙분석(Association Analysis)의 개념
  - 흔히 장바구니분석 또는 서열분석이라고 불림
  - 기업 데이터베이스에서 상품의 구매, 서비스 등 일련의 거래 또는 사건 사이 규칙을 발견하기 위해 적용
  - 장바구니 분석: 장바구니에 무엇이 같이 들어 있는지에 관한 분석
  - 서열 분석: 'A를 구매한 다음에 B를 구매한다'

- 연관규칙의 형태
  - 조건과 반응의 형태(if-then)로 이루어져 있음
  ```
  (Item set A) → (Item set B)
  If A then B: 만일 A가 일어나면 B가 일어난다.
  
  - 아메리카노를 마시는 손님 중 10%가 브라우니를 먹는다.
  - 샌드위치를 먹는 고객의 30%가 탄산수를 함께 마신다.
  ```

- 연관규칙의 측도
  - 산업 특성에 따라 지지도, 신뢰도, 향상도 값을 잘 보고 규칙을 선택해야 함
  
  1. **지지도(support)**
     - 전체 거래 중 항목 A와 항목 B를 동시에 포함하는 거래의 비율
  
  2. **신뢰도(confidence)**
     - 항목 A를 포함한 거래 중에서 항목 A와 항목 B가 같이 포함될 확률 → 연관성 정도 파악 가능
  
  3. **향상도(Lift)**
     - A가 구매되지 않았을 때, 품목 B의 구매확률에 비해 A가 구매됐을 때 품목 B의 구매확률 증가 비
     - 연관규칙 A → B는 품목 A와 품목 B의 구매가 서로 관련 없는 경우에 향상도가 1이 됨

- 연관규칙의 절차
  - 최소 지지도보다 큰 집합만을 대상으로 높은 지지도를 갖는 품목 집합을 찾는 것
  - 처음에는 5%로 잡고 규칙이 충분히 도출되는지 보고 다양하게 조절하여 시도
  - 처음부터 너무 낮은 최소 지지도를 선정하는 것은 많은 리소스가 소모
  - 절차
    - 최소 지지도 결정 → 품목 중 최소 지지도 넘는 품목 분류 → 2가지 품목 집합 생성 → 반복 수행해 빈발품목 집함 찾기
    
- 연관규칙의 장단점

| 장점 | 단점 |
|---|---|
| 조건 반응으로 표현되는 연관선 분석 결과를 쉽게 이해할 수 있음(탐색적 방법), 강력한 비목적성 분석기법으로 분석 방향이나 목적이 특별히 없는 경우 목적변수가 없어 유용하게 활용, 사용이 편리한 분석 데이터 형태로 거래 내용에 관한 데이터를 변환 없이 그 자체로 이용할 수 있는 간단한 자료 구조를 가짐, 분석을 위한 계산이 간단함 | 품목수가 증가하면 분석에 필요한 계산은 기하급수적으로 늘어남,세분화한 품목을 갖고 연관성 규칙을 찾으면 의미 없는 분석이 될 수 있음, 거래량 적은 품목은 당연히 포함된 거래수가 적을 것이고 규칙 발견 시 제외하기가 쉬움 | 

- 순차패턴(Sequence Analysis)
  - 동시에 구매될 가능성이 큰 상품군을 찾는 연관성 분석에, 시간이라는 개념을 포함해 순차적으로 구매 가능성이 큰 상품군을 찾는 것
  - 연관성분석에서의 데이터 형태에서 각각의 고객으로부터 발생한 구매시점에 대한 정보가 포함됨

### 2. 기존 연관성분석의 이슈
- 대용량 데이터에 관한 연관성 분석 불가능
- 시간이 많이 걸리거나 기존 시스템에서 실행 시, 시스템 다운 현상 발생 가능

### 3. 최근 연관성분성 동향
- 1세대 알고리즘인 Apriori나 2세대인 FP-Growth에서 발전하여 3세대의 FPV를 이용해 메모리를 효율적으로 사용 → SKU 레벨인 연관성분석을 성공적으로 적용
- 거래내역에 포함된 모든 품목 개수가 n개일 때, 품목의 전체집합에서 추출할 수 있는 품목 부분집합 개수는 2^n^-1(공집합 제외)개, 가능한 모든 연관규칙 개서는 3^n^ - 2^n+1^ + 1개
- Aprirori: 모든 가능한 품목 부분집합 개수를 줄이는 방식으로 작동
- FP-Growth: 거래내역 안에 포함된 품목 개수를 줄여 비교하는 횟수를 줄이는 방식으로 작동

- **Aprirori 알고리즘**
  - 빈발항목집합: 최소 지지도보다 큰 지지도 값을 갖는 품목 집합
  - 모든 품목집합에 대한 지지도를 전부 계산하지 않고, 최소 지지도 이상의 빈발항목집합을 찾은 후 그것들에 대해서만 연관규칙을 계산하는 것
  - 1994년에 발표된 알고리즘으로 구현과 이해가 쉬우나, 지지도 낮은 후보 집합 생성 시 아이템 개수가 많아지면 계산 복잡도가 증가하는 문제 발생

- **FP-Growth 알고리즘**
  - 후보 빈발항목집합을 생성하지 않고, FP-Tree(Frequent Pattern Tree)를 만든 후 분할정복 방식으로 Apriori 알고리즘보다 더 빠르게 빈발항목집합을 추출할 수 있는 방법
  - Apriori 알고리즘의 약점을 보안하기 위해 고안 → 데이터베이스 스캔 횟수가 작고 빠르게 분석 가능

### 4. 연관성분석 활용방안
- 장바구니 분석의 경우는 실시간 상품추천을 통한 교차판매에 응용
- 순차패턴 분석은 A를 구매한 사람인데 B를 구매하지 않은 경우, B를 추천하는 교차판매 캠페인에 사용

### 5. 연관성분석 예제
- 교재 참고
